{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from operator import mul\n",
    "\n",
    "import cloudpickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import shap\n",
    "from joblib import Memory, Parallel, delayed\n",
    "from loguru import logger as loguru_logger\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wildfires.analysis\n",
    "from alepython import ale_plot\n",
    "from alepython.ale import _second_order_ale_quant\n",
    "from wildfires.analysis import *\n",
    "from wildfires.dask_cx1 import get_parallel_backend\n",
    "from wildfires.data import *\n",
    "from wildfires.logging_config import enable_logging\n",
    "from wildfires.qstat import get_ncpus\n",
    "from wildfires.utils import *\n",
    "\n",
    "loguru_logger.enable(\"alepython\")\n",
    "loguru_logger.remove()\n",
    "loguru_logger.add(sys.stderr, level=\"WARNING\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "enable_logging(\"jupyter\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Collapsing a non-contiguous coordinate.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*DEFAULT_SPHERICAL_EARTH_RADIUS*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*guessing contiguous bounds*\")\n",
    "\n",
    "normal_coast_linewidth = 0.5\n",
    "mpl.rc(\"figure\", figsize=(14, 6))\n",
    "mpl.rc(\"font\", size=9.0)\n",
    "\n",
    "save_name = \"analysis_time_lags_xgboost\"\n",
    "\n",
    "figure_saver = FigureSaver(directories=os.path.join(\"~\", \"tmp\", save_name), debug=True,)\n",
    "memory = get_memory(save_name, verbose=100)\n",
    "CACHE_DIR = os.path.join(DATA_DIR, \".pickle\", save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrite wildfires get_data with our own personalised version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_time_lag_data import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"symlog\"\n",
    "linthres = 1e-2\n",
    "subs = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "log_xscale_kwargs = dict(value=value, linthreshx=linthres, subsx=subs)\n",
    "log_yscale_kwargs = dict(value=value, linthreshy=linthres, subsy=subs)\n",
    "log_vars = (\n",
    "    \"dry day period\",\n",
    "    \"popd\",\n",
    "    \"agb tree\",\n",
    "    \"cape x precip\",\n",
    "    \"lai\",\n",
    "    \"shruball\",\n",
    "    \"pftherb\",\n",
    "    \"pftcrop\",\n",
    "    \"treeall\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ale_2d(predictor, train_set, features, bins=40, coverage=1):\n",
    "    if coverage < 1:\n",
    "        # This should be ok if `train_set` is randomised, as it usually is.\n",
    "        train_set = train_set[: int(train_set.shape[0] * coverage)]\n",
    "    ale, quantiles_list, samples_grid = _second_order_ale_quant(\n",
    "        predictor, train_set, features, bins=bins, return_samples_grid=True\n",
    "    )\n",
    "    fig, ax = plt.subplots()\n",
    "    centres_list = [get_centres(quantiles) for quantiles in quantiles_list]\n",
    "    n_x, n_y = 50, 50\n",
    "    x = np.linspace(centres_list[0][0], centres_list[0][-1], n_x)\n",
    "    y = np.linspace(centres_list[1][0], centres_list[1][-1], n_y)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y, indexing=\"xy\")\n",
    "    ale_interp = scipy.interpolate.interp2d(centres_list[0], centres_list[1], ale.T)\n",
    "    CF = ax.contourf(X, Y, ale_interp(x, y), cmap=\"bwr\", levels=30, alpha=0.7)\n",
    "\n",
    "    # Do not autoscale, so that boxes at the edges (contourf only plots the bin\n",
    "    # centres, not their edges) don't enlarge the plot. Such boxes include markings for\n",
    "    # invalid cells, or hatched boxes for valid cells.\n",
    "    plt.autoscale(False)\n",
    "\n",
    "    # Add hatching for the significant cells. These have at least `min_samples` samples.\n",
    "    # By default, calculate this as the number of samples in each bin if everything was equally distributed, divided by 10.\n",
    "    min_samples = (train_set.shape[0] / reduce(mul, map(len, centres_list))) / 10\n",
    "    for i, j in zip(*np.where(samples_grid >= min_samples)):\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                [quantiles_list[0][i], quantiles_list[1][j]],\n",
    "                quantiles_list[0][i + 1] - quantiles_list[0][i],\n",
    "                quantiles_list[1][j + 1] - quantiles_list[1][j],\n",
    "                linewidth=0,\n",
    "                fill=None,\n",
    "                hatch=\".\",\n",
    "                alpha=0.4,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if np.any(ale.mask):\n",
    "        # Add rectangles to indicate cells without samples.\n",
    "        for i, j in zip(*np.where(ale.mask)):\n",
    "            ax.add_patch(\n",
    "                Rectangle(\n",
    "                    [quantiles_list[0][i], quantiles_list[1][j]],\n",
    "                    quantiles_list[0][i + 1] - quantiles_list[0][i],\n",
    "                    quantiles_list[1][j + 1] - quantiles_list[1][j],\n",
    "                    linewidth=1,\n",
    "                    edgecolor=\"k\",\n",
    "                    facecolor=\"none\",\n",
    "                    alpha=0.4,\n",
    "                )\n",
    "            )\n",
    "    fig.colorbar(CF, format=\"%.0e\")\n",
    "    ax.set_xlabel(features[0])\n",
    "    ax.set_ylabel(features[1])\n",
    "    nbins_str = \"x\".join([str(len(centres)) for centres in centres_list])\n",
    "    ax.set_title(\n",
    "        f\"Second-order ALE of features {features[0]} and {features[1]}\\n\"\n",
    "        f\"Bins: {nbins_str} (Hatching: Sig., Boxes: Invalid)\"\n",
    "    )\n",
    "\n",
    "    if any(log_var.lower() in features[0].lower() for log_var in log_vars):\n",
    "        ax.set_xscale(**log_xscale_kwargs)\n",
    "    if any(log_var.lower() in features[1].lower() for log_var in log_vars):\n",
    "        ax.set_yscale(**log_yscale_kwargs)\n",
    "    figure_saver.save_figure(fig, \"__\".join(columns), sub_directory=\"2d_ale_low\")\n",
    "    return ale, quantiles_list, samples_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data Structures used for Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_months = [1, 3, 6, 9, 12, 18, 24]\n",
    "\n",
    "# selection_variables = (\n",
    "#     \"VOD Ku-band -3 Month\",\n",
    "#     # \"SIF\",  # Fix regridding!!\n",
    "#     \"VOD Ku-band -1 Month\",\n",
    "#     \"Dry Day Period -3 Month\",\n",
    "#     \"FAPAR\",\n",
    "#     \"pftHerb\",\n",
    "#     \"LAI -1 Month\",\n",
    "#     \"popd\",\n",
    "#     \"Dry Day Period -24 Month\",\n",
    "#     \"pftCrop\",\n",
    "#     \"FAPAR -1 Month\",\n",
    "#     \"FAPAR -24 Month\",\n",
    "#     \"Max Temp\",\n",
    "#     \"Dry Day Period -6 Month\",\n",
    "#     \"VOD Ku-band -6 Month\",\n",
    "# )\n",
    "\n",
    "# ext_selection_variables = selection_variables + (\n",
    "#     \"Dry Day Period -1 Month\",\n",
    "#     \"FAPAR -6 Month\",\n",
    "#     \"ShrubAll\",\n",
    "#     \"SWI(1)\",\n",
    "#     \"TreeAll\",\n",
    "# )\n",
    "from ipdb import launch_ipdb_on_exception\n",
    "\n",
    "with launch_ipdb_on_exception():\n",
    "    (\n",
    "        e_s_endog_data,\n",
    "        e_s_exog_data,\n",
    "        e_s_master_mask,\n",
    "        e_s_filled_datasets,\n",
    "        e_s_masked_datasets,\n",
    "        e_s_land_mask,\n",
    "    ) = get_data(shift_months=shift_months, selection_variables=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset data from 12 or more months before the current month in order to ease analysis (interpretability).\n",
    "We are interested in the trends in these properties, not their absolute values, therefore we subtract a recent 'seasonal cycle' analogue.\n",
    "This hopefully avoids capturing the same relationships for a variable and its 12 month counterpart due to their high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = []\n",
    "for column in e_s_exog_data:\n",
    "    match = re.search(r\"-\\d{1,2}\", column)\n",
    "    if match:\n",
    "        span = match.span()\n",
    "        # Change the string to reflect the shift.\n",
    "        original_offset = int(column[slice(*span)])\n",
    "        if original_offset > -12:\n",
    "            # Only shift months that are 12 or more months before the current month.\n",
    "            continue\n",
    "        comp = -(-original_offset % 12)\n",
    "        new_column = \" \".join(\n",
    "            (\n",
    "                column[: span[0] - 1],\n",
    "                f\"{original_offset} - {comp}\",\n",
    "                column[span[1] + 1 :],\n",
    "            )\n",
    "        )\n",
    "        if comp == 0:\n",
    "            comp_column = column[: span[0] - 1]\n",
    "        else:\n",
    "            comp_column = \" \".join(\n",
    "                (column[: span[0] - 1], f\"{comp}\", column[span[1] + 1 :])\n",
    "            )\n",
    "        print(column, comp_column)\n",
    "        e_s_exog_data[new_column] = e_s_exog_data[column] - e_s_exog_data[comp_column]\n",
    "        to_delete.append(column)\n",
    "for column in to_delete:\n",
    "    del e_s_exog_data[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cached Model Fitting\n",
    "If anything regarding the data changes above, the cache has to be refreshed using memory.clear()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import *\n",
    "\n",
    "client = Client(n_workers=1, threads_per_worker=get_ncpus())\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_xgboost\n",
    "import xgboost as xgb\n",
    "from dask import array as darray\n",
    "from dask import dataframe as dd\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from wildfires.dask_cx1 import *\n",
    "\n",
    "ModelResults = namedtuple(\n",
    "    \"ModelResults\",\n",
    "    (\n",
    "        \"X_train\",\n",
    "        \"X_test\",\n",
    "        \"y_train\",\n",
    "        \"y_test\",\n",
    "        \"r2_test\",\n",
    "        \"r2_train\",\n",
    "        \"model\",\n",
    "        \"mse_train\",\n",
    "        \"mse_test\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "model_cache = SimpleCache(\"xgboost_model\", cache_dir=CACHE_DIR)\n",
    "\n",
    "# XXX:\n",
    "model_cache.clear()\n",
    "\n",
    "\n",
    "def get_darray(numpy_array, chunk_nrows):\n",
    "    if len(numpy_array.shape) == 1:\n",
    "        return darray.from_array(numpy_array, chunks=(chunk_nrows,))\n",
    "    elif len(numpy_array.shape) == 2:\n",
    "        return darray.from_array(numpy_array, chunks=(chunk_nrows, -1))\n",
    "    raise ValueError(\"Expected (m,) or (m, n) array.\")\n",
    "\n",
    "\n",
    "@model_cache\n",
    "def get_time_lags_model():\n",
    "    \"\"\"Get an XGBOOST model trained on the extended shifted data.\n",
    "    \n",
    "    Returns:\n",
    "        ModelResults: A namedtuple with the fields 'X_train', 'X_test', 'y_train', 'y_test', \n",
    "        'r2_test', 'r2_train', and 'model'.\n",
    "    \n",
    "    \"\"\"\n",
    "    N = slice(0, 10000)\n",
    "\n",
    "    # Split the data.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        e_s_exog_data[N], e_s_endog_data[N], random_state=1, shuffle=True, test_size=0.3\n",
    "    )\n",
    "    # Define and train the model.\n",
    "    # client = get_client()\n",
    "\n",
    "    # Used to define the chunk size.\n",
    "    n_cores = 20\n",
    "\n",
    "    chunk_nrows = max((int(X_train.shape[0] / (n_cores * 3)), 20))\n",
    "\n",
    "    da_X_train = get_darray(X_train.values, chunk_nrows)\n",
    "    da_y_train = get_darray(y_train.values, chunk_nrows)\n",
    "\n",
    "    da_X_test = get_darray(X_test.values, chunk_nrows)\n",
    "    da_y_test = get_darray(y_test.values, chunk_nrows)\n",
    "\n",
    "    regressor = xgb.dask.DaskXGBRegressor(n_estimators=10000, max_depth=10,)\n",
    "    #     regressor.set_params(**\n",
    "    #         {\n",
    "    #             # 'tree_method': 'hist',\n",
    "    #             # 'grow_policy': 'lossguide',\n",
    "    #             # 'eta': 0.3,\n",
    "    #             # 'min_child_weight': 1,\n",
    "    #             # 'subsample': 1,\n",
    "    #             # 'colsample_bytree': 1,\n",
    "    #         }),\n",
    "    # Apparently this is optional - a 'global client' is used if not given.\n",
    "    regressor.client = client\n",
    "\n",
    "    #\n",
    "    # Need to combat overfitting!!!\n",
    "    #\n",
    "    # The dask-ml version might support early stopping!\n",
    "    # https://ml.dask.org/modules/generated/dask_ml.xgboost.XGBRegressor.html\n",
    "    regressor.fit(\n",
    "        da_X_train,\n",
    "        da_y_train,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=True,\n",
    "        eval_set=[(da_X_test, da_y_test)],\n",
    "    )\n",
    "\n",
    "    bst = regressor.get_booster()\n",
    "    history = regressor.evals_result()\n",
    "    # print('Evaluation history:', history)\n",
    "\n",
    "    y_test_pred = regressor.predict(da_X_test)\n",
    "    y_train_pred = regressor.predict(da_X_train)\n",
    "\n",
    "    mse_test = mean_squared_error(y_test_pred, y_test)\n",
    "    mse_train = mean_squared_error(y_train_pred, y_train)\n",
    "\n",
    "    r2_test = r2_score(y_test_pred, y_test)\n",
    "    r2_train = r2_score(y_train_pred, y_train)\n",
    "\n",
    "    return ModelResults(\n",
    "        X_train, X_test, y_train, y_test, r2_test, r2_train, bst, mse_train, mse_test\n",
    "    )\n",
    "\n",
    "\n",
    "model_results = get_time_lags_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 train:\", model_results.r2_train)\n",
    "print(\"R2 test:\", model_results.r2_test)\n",
    "\n",
    "print(\"mse train:\", model_results.mse_train)\n",
    "print(\"mse test:\", model_results.mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model_results.model.predict(\n",
    "    dd.from_pandas(model_results.X_train, npartitions=100)\n",
    ").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_results.y_train[:100], y_train_pred[:100], linestyle=\"\", marker=\"o\")\n",
    "_ = plt.gca().set(xlabel=\"Train\", ylabel=\"Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values\n",
    "Using dask to parallelise the SHAP value calculations is possible (sometimes), but VERY unstable, just like local backends (eg. loky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_results.model, data=model_results.X_train[-100:])\n",
    "with Time(\"100, 100 vals\"):\n",
    "    shap_values = explainer.shap_values(model_results.X_train[:100])\n",
    "shap.summary_plot(shap_values, model_results.X_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_results.model, data=model_results.X_train[-200:])\n",
    "with Time(\"200, 100 vals\"):\n",
    "    shap_values = explainer.shap_values(model_results.X_train[:100])\n",
    "shap.summary_plot(shap_values, model_results.X_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_results.model, data=model_results.X_train[-400:])\n",
    "with Time(\"400, 100 vals\"):\n",
    "    shap_values = explainer.shap_values(model_results.X_train[:100])\n",
    "shap.summary_plot(shap_values, model_results.X_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_results.model, data=model_results.X_train[-400:])\n",
    "with Time(\"400, 500 vals\"):\n",
    "    shap_values = explainer.shap_values(model_results.X_train[:500])\n",
    "shap.summary_plot(shap_values, model_results.X_train[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_results.model, data=model_results.X_train[-500:])\n",
    "with Time(\"500, 2000 vals\"):\n",
    "    shap_values = explainer.shap_values(model_results.X_train[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, model_results.X_train[:2000], max_display=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_results.model)\n",
    "N = 100\n",
    "with Time(f\"None, {N} vals\"):\n",
    "    shap_values = explainer.shap_values(model_results.X_train[:N])\n",
    "shap.summary_plot(shap_values, model_results.X_train[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Interaction Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_results.model)\n",
    "N = 10\n",
    "with Time(f\"shap interaction, {N}\"):\n",
    "    shap_interaction_values = explainer.shap_interaction_values(\n",
    "        model_results.X_train[:N]\n",
    "    )\n",
    "shap.summary_plot(shap_values, model_results.X_train[:N])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
