{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unrestricted max depth RF model ALE plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from operator import mul\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from joblib import Memory\n",
    "from loguru import logger as loguru_logger\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wildfires.analysis\n",
    "from alepython import ale_plot\n",
    "from alepython.ale import _second_order_ale_quant\n",
    "from wildfires.analysis import *\n",
    "from wildfires.dask_cx1 import get_client\n",
    "from wildfires.data import *\n",
    "from wildfires.logging_config import enable_logging\n",
    "from wildfires.qstat import get_ncpus\n",
    "from wildfires.utils import *\n",
    "\n",
    "loguru_logger.enable(\"alepython\")\n",
    "loguru_logger.remove()\n",
    "loguru_logger.add(sys.stderr, level=\"WARNING\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "enable_logging(\"jupyter\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Collapsing a non-contiguous coordinate.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*DEFAULT_SPHERICAL_EARTH_RADIUS*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*guessing contiguous bounds*\")\n",
    "\n",
    "normal_coast_linewidth = 0.5\n",
    "mpl.rc(\"figure\", figsize=(14, 6))\n",
    "mpl.rc(\"font\", size=9.0)\n",
    "\n",
    "figure_saver = FigureSaver(\n",
    "    directories=os.path.join(\"~\", \"tmp\", \"analysis_multiple_time_lags_model\"),\n",
    "    debug=True,\n",
    ")\n",
    "memory = get_memory(\"analysis_multiple_time_lags_model\", verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"symlog\"\n",
    "linthres = 1e-2\n",
    "subs = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "log_xscale_kwargs = dict(value=value, linthreshx=linthres, subsx=subs)\n",
    "log_yscale_kwargs = dict(value=value, linthreshy=linthres, subsy=subs)\n",
    "log_vars = (\n",
    "    \"dry day period\",\n",
    "    \"popd\",\n",
    "    \"agb tree\",\n",
    "    \"cape x precip\",\n",
    "    \"lai\",\n",
    "    \"shruball\",\n",
    "    \"pftherb\",\n",
    "    \"pftcrop\",\n",
    "    \"treeall\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ale_2d(predictor, train_set, features, bins=40, coverage=1):\n",
    "    if coverage < 1:\n",
    "        # This should be ok if `train_set` is randomised, as it usually is.\n",
    "        train_set = train_set[: int(train_set.shape[0] * coverage)]\n",
    "    ale, quantiles_list, samples_grid = _second_order_ale_quant(\n",
    "        predictor, train_set, features, bins=bins, return_samples_grid=True\n",
    "    )\n",
    "    fig, ax = plt.subplots()\n",
    "    centres_list = [get_centres(quantiles) for quantiles in quantiles_list]\n",
    "    n_x, n_y = 50, 50\n",
    "    x = np.linspace(centres_list[0][0], centres_list[0][-1], n_x)\n",
    "    y = np.linspace(centres_list[1][0], centres_list[1][-1], n_y)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y, indexing=\"xy\")\n",
    "    ale_interp = scipy.interpolate.interp2d(centres_list[0], centres_list[1], ale.T)\n",
    "    CF = ax.contourf(X, Y, ale_interp(x, y), cmap=\"bwr\", levels=30, alpha=0.7)\n",
    "\n",
    "    # Do not autoscale, so that boxes at the edges (contourf only plots the bin\n",
    "    # centres, not their edges) don't enlarge the plot. Such boxes include markings for\n",
    "    # invalid cells, or hatched boxes for valid cells.\n",
    "    plt.autoscale(False)\n",
    "\n",
    "    # Add hatching for the significant cells. These have at least `min_samples` samples.\n",
    "    # By default, calculate this as the number of samples in each bin if everything was equally distributed, divided by 10.\n",
    "    min_samples = (train_set.shape[0] / reduce(mul, map(len, centres_list))) / 10\n",
    "    for i, j in zip(*np.where(samples_grid >= min_samples)):\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                [quantiles_list[0][i], quantiles_list[1][j]],\n",
    "                quantiles_list[0][i + 1] - quantiles_list[0][i],\n",
    "                quantiles_list[1][j + 1] - quantiles_list[1][j],\n",
    "                linewidth=0,\n",
    "                fill=None,\n",
    "                hatch=\".\",\n",
    "                alpha=0.4,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if np.any(ale.mask):\n",
    "        # Add rectangles to indicate cells without samples.\n",
    "        for i, j in zip(*np.where(ale.mask)):\n",
    "            ax.add_patch(\n",
    "                Rectangle(\n",
    "                    [quantiles_list[0][i], quantiles_list[1][j]],\n",
    "                    quantiles_list[0][i + 1] - quantiles_list[0][i],\n",
    "                    quantiles_list[1][j + 1] - quantiles_list[1][j],\n",
    "                    linewidth=1,\n",
    "                    edgecolor=\"k\",\n",
    "                    facecolor=\"none\",\n",
    "                    alpha=0.4,\n",
    "                )\n",
    "            )\n",
    "    fig.colorbar(CF, format=\"%.0e\")\n",
    "    ax.set_xlabel(features[0])\n",
    "    ax.set_ylabel(features[1])\n",
    "    nbins_str = \"x\".join([str(len(centres)) for centres in centres_list])\n",
    "    ax.set_title(\n",
    "        f\"Second-order ALE of features {features[0]} and {features[1]}\\n\"\n",
    "        f\"Bins: {nbins_str} (Hatching: Sig., Boxes: Invalid)\"\n",
    "    )\n",
    "\n",
    "    if any(log_var.lower() in features[0].lower() for log_var in log_vars):\n",
    "        ax.set_xscale(**log_xscale_kwargs)\n",
    "    if any(log_var.lower() in features[1].lower() for log_var in log_vars):\n",
    "        ax.set_yscale(**log_yscale_kwargs)\n",
    "    figure_saver.save_figure(fig, \"__\".join(features), sub_directory=\"2d_ale\")\n",
    "    return ale, quantiles_list, samples_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data Structures used for Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_months = [1, 3, 6, 9, 12, 18, 24]\n",
    "\n",
    "# selection_variables = (\n",
    "#     \"VOD Ku-band -3 Month\",\n",
    "#     # \"SIF\",  # Fix regridding!!\n",
    "#     \"VOD Ku-band -1 Month\",\n",
    "#     \"Dry Day Period -3 Month\",\n",
    "#     \"FAPAR\",\n",
    "#     \"pftHerb\",\n",
    "#     \"LAI -1 Month\",\n",
    "#     \"popd\",\n",
    "#     \"Dry Day Period -24 Month\",\n",
    "#     \"pftCrop\",\n",
    "#     \"FAPAR -1 Month\",\n",
    "#     \"FAPAR -24 Month\",\n",
    "#     \"Max Temp\",\n",
    "#     \"Dry Day Period -6 Month\",\n",
    "#     \"VOD Ku-band -6 Month\",\n",
    "# )\n",
    "\n",
    "# ext_selection_variables = selection_variables + (\n",
    "#     \"Dry Day Period -1 Month\",\n",
    "#     \"FAPAR -6 Month\",\n",
    "#     \"ShrubAll\",\n",
    "#     \"SWI(1)\",\n",
    "#     \"TreeAll\",\n",
    "# )\n",
    "from ipdb import launch_ipdb_on_exception\n",
    "\n",
    "with launch_ipdb_on_exception():\n",
    "    (\n",
    "        e_s_endog_data,\n",
    "        e_s_exog_data,\n",
    "        e_s_master_mask,\n",
    "        e_s_filled_datasets,\n",
    "        e_s_masked_datasets,\n",
    "        e_s_land_mask,\n",
    "    ) = wildfires.analysis.time_lags.get_data(\n",
    "        shift_months=shift_months, selection_variables=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset data that has come 12 or more months before the current month in order to ease analysis.\n",
    "We are interested in the trends in these properties, not their absolute values, therefore we subtract a recent 'seasonal cycle' analogue.\n",
    "This hopefully avoids capturing the same relationships for a variable and its 12 month counterpart due to their high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = []\n",
    "for column in e_s_exog_data:\n",
    "    match = re.search(r\"-\\d{1,2}\", column)\n",
    "    if match:\n",
    "        span = match.span()\n",
    "        # Change the string to reflect the shift.\n",
    "        original_offset = int(column[slice(*span)])\n",
    "        if original_offset > -12:\n",
    "            # Only shift months that are 12 or more months before the current month.\n",
    "            continue\n",
    "        comp = -(-original_offset % 12)\n",
    "        new_column = \" \".join(\n",
    "            (\n",
    "                column[: span[0] - 1],\n",
    "                f\"{original_offset} - {comp}\",\n",
    "                column[span[1] + 1 :],\n",
    "            )\n",
    "        )\n",
    "        if comp == 0:\n",
    "            comp_column = column[: span[0] - 1]\n",
    "        else:\n",
    "            comp_column = \" \".join(\n",
    "                (column[: span[0] - 1], f\"{comp}\", column[span[1] + 1 :])\n",
    "            )\n",
    "        print(column, comp_column)\n",
    "        e_s_exog_data[new_column] = e_s_exog_data[column] - e_s_exog_data[comp_column]\n",
    "        to_delete.append(column)\n",
    "for column in to_delete:\n",
    "    del e_s_exog_data[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cached Model Fitting\n",
    "If anything regarding the data changes above, the cache has to be refreshed using memory.clear()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelResults = namedtuple(\n",
    "    \"ModelResults\",\n",
    "    (\"X_train\", \"X_test\", \"y_train\", \"y_test\", \"r2_test\", \"r2_train\", \"model\"),\n",
    ")\n",
    "\n",
    "\n",
    "@memory.cache()\n",
    "def get_time_lags_model():\n",
    "    \"\"\"Get a RF model trained on the extended shifted data.\n",
    "    \n",
    "    Returns:\n",
    "        ModelResults: A namedtuple with the fields 'X_train', 'X_test', 'y_train', 'y_test', \n",
    "        'r2_test', 'r2_train', and 'model'.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Split the data.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        e_s_exog_data, e_s_endog_data, random_state=1, shuffle=True, test_size=0.3\n",
    "    )\n",
    "    # Define and train the model.\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=None, random_state=1, n_jobs=get_ncpus(),\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    r2_test = rf.score(X_test, y_test)\n",
    "    r2_train = rf.score(X_train, y_train)\n",
    "\n",
    "    return ModelResults(X_train, X_test, y_train, y_test, r2_test, r2_train, rf)\n",
    "\n",
    "\n",
    "model_results = get_time_lags_model()\n",
    "# Take advantage of all cores available to our job.\n",
    "model_results.model.n_jobs = get_ncpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_results.model.feature_importances_\n",
    "std = np.std(\n",
    "    [tree.feature_importances_ for tree in model_results.model.estimators_], axis=0\n",
    ")\n",
    "\n",
    "importances_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Name\": model_results.X_train.columns.values,\n",
    "        \"Importance\": importances,\n",
    "        \"Importance STD\": std,\n",
    "        \"Ratio\": np.array(std) / np.array(importances),\n",
    "    }\n",
    ")\n",
    "print(\n",
    "    \"\\n\"\n",
    "    + str(\n",
    "        importances_df.sort_values(\"Importance\", ascending=False).to_string(\n",
    "            index=False, float_format=\"{:0.3f}\".format, line_width=200\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worldwide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D ALE interaction plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"localhost:35494\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f = client.scatter(model_results.model, broadcast=True)\n",
    "model_X = client.scatter(model_results.X_train, broadcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = 0.5\n",
    "\n",
    "\n",
    "def plot_ale_and_get_importance(columns, model, train_set):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    enable_logging(\"jupyter\")\n",
    "    mpl.rc(\"figure\", figsize=(14, 6))\n",
    "    mpl.rc(\"font\", size=9.0)\n",
    "    try:\n",
    "        model.n_jobs = get_ncpus()\n",
    "        ale, quantiles_list, samples_grid = ale_2d(\n",
    "            model.predict, train_set, columns, bins=20, coverage=coverage,\n",
    "        )\n",
    "        min_samples = (\n",
    "            train_set.shape[0] / reduce(mul, map(lambda x: len(x) - 1, quantiles_list))\n",
    "        ) / 10\n",
    "        return np.ma.max(ale[samples_grid > min_samples]) - np.ma.min(\n",
    "            ale[samples_grid > min_samples]\n",
    "        )\n",
    "    except:\n",
    "        logger.exception(\n",
    "            f\"Something went wrong with 2D ALE plotting for columns {columns}.\"\n",
    "        )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for columns in tqdm(, desc=\"Calculating 2D ALE plots\"):\n",
    "columns_list = list(combinations(model_results.X_train.columns, 2))\n",
    "ptp_values = client.gather(\n",
    "    client.map(\n",
    "        plot_ale_and_get_importance,\n",
    "        *list(zip(*((columns, model_f, model_X) for columns in columns_list)))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore and count None values, then plot a histogram of the ptp values.\n",
    "filtered_columns_list = []\n",
    "filtered_ptp_values = []\n",
    "invalid_columns = []\n",
    "for columns, ptp in zip(columns_list, ptp_values):\n",
    "    if ptp is not None:\n",
    "        filtered_columns_list.append(columns)\n",
    "        filtered_ptp_values.append(ptp)\n",
    "    else:\n",
    "        invalid_columns.append(columns)\n",
    "\n",
    "print(\"Nr invalid:\", len(invalid_columns))\n",
    "print(pd.Series(invalid_columns))\n",
    "\n",
    "np.asarray([ptp for ptp in ptp_values if ptp is not None])\n",
    "_ = plt.hist(filtered_ptp_values, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_results = pd.Series(filtered_ptp_values, index=filtered_columns_list)\n",
    "pdp_results.sort_values(inplace=True, ascending=False)\n",
    "print(pdp_results.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in tqdm(model_results.X_train.columns, desc=\"Calculating ALE plots\"):\n",
    "    with figure_saver(column, sub_directory=\"ale\"):\n",
    "        ale_plot(\n",
    "            model_results.model,\n",
    "            model_results.X_train,\n",
    "            column,\n",
    "            bins=40,\n",
    "            monte_carlo=True,\n",
    "            monte_carlo_rep=100,\n",
    "            monte_carlo_ratio=0.01,\n",
    "            plot_quantiles=False,\n",
    "        )\n",
    "        plt.gcf().axes[0].lines[-1].set_marker(\".\")\n",
    "        if any(feature.lower() in column.lower() for feature in log_vars):\n",
    "            plt.gcf().axes[0].set_xscale(**log_xscale_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the original DataFrame to analyse specific regions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataframe(data, original_mask, additional_mask, suffix=\"\"):\n",
    "    \"\"\"Sub-set results based on an additional mask.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.core.frame.DataFrame): Data to select.\n",
    "        orig_mask (array-like): Original mask that was used to transform the data into the column representation in `data`. This mask should be False where data should be selected.\n",
    "        additional_mask (array-like): After conversion of columns in `data` back to a lat-lon grid, this mask will be used in addition to `orig_mask` to return a subset of the data to the column format. This mask should be False where data should be selected.\n",
    "        suffix (str): Suffix to add to column labels. An empty space will be added to the beginning of `suffix` if this is not already present.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.core.frame.DataFrame: Selected data.\n",
    "    \n",
    "    \"\"\"\n",
    "    additional_mask = match_shape(additional_mask, original_mask.shape)\n",
    "    if suffix:\n",
    "        if suffix[0] != \" \":\n",
    "            suffix = \" \" + suffix\n",
    "    new_data = {}\n",
    "    for column in tqdm(data.columns, desc=\"Selecting data\"):\n",
    "        # Create a blank lat-lon grid.\n",
    "        lat_lon_data = np.empty_like(original_mask, dtype=np.float64)\n",
    "        # Convert data from the dense column representation to the sparse lat-lon grid.\n",
    "        lat_lon_data[~original_mask] = data[column]\n",
    "        # Use the original and the new mask to create new columns.\n",
    "        new_data[column + suffix] = lat_lon_data[\n",
    "            ((~original_mask) & (~additional_mask))\n",
    "        ]\n",
    "    return pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SE Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new mask.\n",
    "region_mask = ~box_mask(lats=(-10, 10), lons=(95, 150))\n",
    "cube_plotting(region_mask)\n",
    "\n",
    "# XXX: This only allows subsetting the original data (both training and test!) since otherwise the original mask does not apply.\n",
    "\n",
    "# Apply new mask.\n",
    "sub_X = subset_dataframe(e_s_exog_data, e_s_master_mask, region_mask, \"SE ASIA\")\n",
    "print(\"Original size:\", e_s_exog_data.shape)\n",
    "print(\"Selected size:\", sub_X.shape)\n",
    "\n",
    "# Plot ALE plots for only this region.\n",
    "for column in tqdm(sub_X.columns, desc=\"Calculating ALE plots\"):\n",
    "    with figure_saver(column, sub_directory=\"ale_se_asia\"):\n",
    "        ale_plot(\n",
    "            model_results.model,\n",
    "            sub_X,\n",
    "            column,\n",
    "            bins=40,\n",
    "            monte_carlo=True,\n",
    "            monte_carlo_rep=30,\n",
    "            monte_carlo_ratio=0.1,\n",
    "            verbose=False,\n",
    "            log=\"x\"\n",
    "            if any(\n",
    "                feature.lower() in column.lower()\n",
    "                for feature in (\"dry day period\", \"popd\",)\n",
    "            )\n",
    "            else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brazilian Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new mask.\n",
    "region_mask = ~box_mask(lats=(-15, 1), lons=(-72, -46))\n",
    "cube_plotting(region_mask)\n",
    "\n",
    "# XXX: This only allows subsetting the original data (both training and test!) since otherwise the original mask does not apply.\n",
    "\n",
    "# Apply new mask.\n",
    "sub_X = subset_dataframe(e_s_exog_data, e_s_master_mask, region_mask, \"BRAZ AMAZ\")\n",
    "print(\"Original size:\", e_s_exog_data.shape)\n",
    "print(\"Selected size:\", sub_X.shape)\n",
    "\n",
    "# Plot ALE plots for only this region.\n",
    "for column in tqdm(sub_X.columns, desc=\"Calculating ALE plots\"):\n",
    "    with figure_saver(column, sub_directory=\"ale_braz_amaz\"):\n",
    "        ale_plot(\n",
    "            model_results.model,\n",
    "            sub_X,\n",
    "            column,\n",
    "            bins=40,\n",
    "            monte_carlo=True,\n",
    "            monte_carlo_rep=30,\n",
    "            monte_carlo_ratio=0.1,\n",
    "            verbose=False,\n",
    "            log=\"x\"\n",
    "            if any(\n",
    "                feature.lower() in column.lower()\n",
    "                for feature in (\"dry day period\", \"popd\",)\n",
    "            )\n",
    "            else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new mask.\n",
    "region_mask = ~box_mask(lats=(33, 73), lons=(-11, 29))\n",
    "cube_plotting(region_mask)\n",
    "\n",
    "# XXX: This only allows subsetting the original data (both training and test!) since otherwise the original mask does not apply.\n",
    "\n",
    "# Apply new mask.\n",
    "sub_X = subset_dataframe(e_s_exog_data, e_s_master_mask, region_mask, \"EUROPE\")\n",
    "print(\"Original size:\", e_s_exog_data.shape)\n",
    "print(\"Selected size:\", sub_X.shape)\n",
    "\n",
    "# Plot ALE plots for only this region.\n",
    "for column in tqdm(sub_X.columns, desc=\"Calculating ALE plots\"):\n",
    "    with figure_saver(column, sub_directory=\"ale_europe\"):\n",
    "        ale_plot(\n",
    "            model_results.model,\n",
    "            sub_X,\n",
    "            column,\n",
    "            bins=40,\n",
    "            monte_carlo=True,\n",
    "            monte_carlo_rep=30,\n",
    "            monte_carlo_ratio=0.1,\n",
    "            verbose=False,\n",
    "            log=\"x\"\n",
    "            if any(\n",
    "                feature.lower() in column.lower()\n",
    "                for feature in (\"dry day period\", \"popd\",)\n",
    "            )\n",
    "            else None,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
