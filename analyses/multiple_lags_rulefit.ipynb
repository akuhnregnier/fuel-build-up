{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifted (and relative) data with RuleFit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from operator import mul\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from joblib import Memory, parallel_backend\n",
    "from loguru import logger as loguru_logger\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wildfires.analysis\n",
    "from alepython import ale_plot\n",
    "from alepython.ale import _second_order_ale_quant\n",
    "from rulefit import RuleFit\n",
    "from wildfires.analysis import *\n",
    "from wildfires.dask_cx1 import get_client\n",
    "from wildfires.data import *\n",
    "from wildfires.logging_config import enable_logging\n",
    "from wildfires.qstat import get_ncpus\n",
    "from wildfires.utils import *\n",
    "\n",
    "loguru_logger.enable(\"alepython\")\n",
    "loguru_logger.remove()\n",
    "loguru_logger.add(sys.stderr, level=\"WARNING\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "enable_logging(\"jupyter\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Collapsing a non-contiguous coordinate.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*DEFAULT_SPHERICAL_EARTH_RADIUS*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*guessing contiguous bounds*\")\n",
    "\n",
    "normal_coast_linewidth = 0.5\n",
    "mpl.rc(\"figure\", figsize=(14, 6))\n",
    "mpl.rc(\"font\", size=9.0)\n",
    "\n",
    "figure_saver = FigureSaver(\n",
    "    directories=os.path.join(\"~\", \"tmp\", \"analysis_multiple_lags_rulefit\"), debug=True\n",
    ")\n",
    "memory = get_memory(\"analysis_multiple_lags_rulefit\", verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"symlog\"\n",
    "linthres = 1e-2\n",
    "subs = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "log_xscale_kwargs = dict(value=value, linthreshx=linthres, subsx=subs)\n",
    "log_yscale_kwargs = dict(value=value, linthreshy=linthres, subsy=subs)\n",
    "log_vars = (\n",
    "    \"dry day period\",\n",
    "    \"popd\",\n",
    "    \"agb tree\",\n",
    "    \"cape x precip\",\n",
    "    \"lai\",\n",
    "    \"shruball\",\n",
    "    \"pftherb\",\n",
    "    \"pftcrop\",\n",
    "    \"treeall\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data Structures used for Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_months = [1, 3, 6, 9, 12, 18, 24]\n",
    "\n",
    "# selection_variables = (\n",
    "#     \"VOD Ku-band -3 Month\",\n",
    "#     # \"SIF\",  # Fix regridding!!\n",
    "#     \"VOD Ku-band -1 Month\",\n",
    "#     \"Dry Day Period -3 Month\",\n",
    "#     \"FAPAR\",\n",
    "#     \"pftHerb\",\n",
    "#     \"LAI -1 Month\",\n",
    "#     \"popd\",\n",
    "#     \"Dry Day Period -24 Month\",\n",
    "#     \"pftCrop\",\n",
    "#     \"FAPAR -1 Month\",\n",
    "#     \"FAPAR -24 Month\",\n",
    "#     \"Max Temp\",\n",
    "#     \"Dry Day Period -6 Month\",\n",
    "#     \"VOD Ku-band -6 Month\",\n",
    "# )\n",
    "\n",
    "# ext_selection_variables = selection_variables + (\n",
    "#     \"Dry Day Period -1 Month\",\n",
    "#     \"FAPAR -6 Month\",\n",
    "#     \"ShrubAll\",\n",
    "#     \"SWI(1)\",\n",
    "#     \"TreeAll\",\n",
    "# )\n",
    "from ipdb import launch_ipdb_on_exception\n",
    "\n",
    "with launch_ipdb_on_exception():\n",
    "    (\n",
    "        e_s_endog_data,\n",
    "        e_s_exog_data,\n",
    "        e_s_master_mask,\n",
    "        e_s_filled_datasets,\n",
    "        e_s_masked_datasets,\n",
    "        e_s_land_mask,\n",
    "    ) = wildfires.analysis.time_lags.get_data(\n",
    "        shift_months=shift_months, selection_variables=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset data that has come 12 or more months before the current month in order to ease analysis.\n",
    "We are interested in the trends in these properties, not their absolute values, therefore we subtract a recent 'seasonal cycle' analogue.\n",
    "This hopefully avoids capturing the same relationships for a variable and its 12 month counterpart due to their high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = []\n",
    "for column in e_s_exog_data:\n",
    "    match = re.search(r\"-\\d{1,2}\", column)\n",
    "    if match:\n",
    "        span = match.span()\n",
    "        # Change the string to reflect the shift.\n",
    "        original_offset = int(column[slice(*span)])\n",
    "        if original_offset > -12:\n",
    "            # Only shift months that are 12 or more months before the current month.\n",
    "            continue\n",
    "        comp = -(-original_offset % 12)\n",
    "        new_column = \" \".join(\n",
    "            (\n",
    "                column[: span[0] - 1],\n",
    "                f\"{original_offset} - {comp}\",\n",
    "                column[span[1] + 1 :],\n",
    "            )\n",
    "        )\n",
    "        if comp == 0:\n",
    "            comp_column = column[: span[0] - 1]\n",
    "        else:\n",
    "            comp_column = \" \".join(\n",
    "                (column[: span[0] - 1], f\"{comp}\", column[span[1] + 1 :])\n",
    "            )\n",
    "        print(column, comp_column)\n",
    "        e_s_exog_data[new_column] = e_s_exog_data[column] - e_s_exog_data[comp_column]\n",
    "        to_delete.append(column)\n",
    "for column in to_delete:\n",
    "    del e_s_exog_data[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the RuleFit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_pickle = os.path.join(DATA_DIR, \".pickle\", \"rulefit_03_05_2020_gb_default\")\n",
    "if os.path.isfile(model_pickle):\n",
    "    print(\"Loading files\")\n",
    "    with open(model_pickle, \"rb\") as f:\n",
    "        pass\n",
    "        # rf, X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "else:\n",
    "    print(\"Fitting model\")\n",
    "    # Split the data.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        e_s_exog_data, e_s_endog_data, random_state=1, shuffle=True, test_size=0.3\n",
    "    )\n",
    "    rf = RuleFit()\n",
    "    # Fit the model.\n",
    "    rf.fit(X_train.values, y_train.values, feature_names=X_train.columns)\n",
    "    print(\"Writing files to disk\")\n",
    "    os.makedirs(os.path.dirname(model_pickle), exist_ok=True)\n",
    "    with open(model_pickle, \"wb\") as f:\n",
    "        pickle.dump((rf, X_train, X_test, y_train, y_test), f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "train_pred_y = rf.predict(X_train.values)\n",
    "test_pred_y = rf.predict(X_test.values)\n",
    "train_r2 = r2_score(y_true=y_train, y_pred=train_pred_y)\n",
    "test_r2 = r2_score(y_true=y_test, y_pred=test_pred_y)\n",
    "print(\"train R2\", train_r2)\n",
    "print(\"test R2\", test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = rf.get_rules()\n",
    "rules = rules[rules.coef != 0].sort_values([\"support\", \"importance\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 900\n",
    "pd.options.display.max_colwidth = 300\n",
    "rules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires]",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
