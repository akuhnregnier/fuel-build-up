{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from operator import mul\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from joblib import Memory, Parallel, delayed\n",
    "from loguru import logger as loguru_logger\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wildfires.analysis\n",
    "from alepython import ale_plot\n",
    "from alepython.ale import _second_order_ale_quant\n",
    "from wildfires.analysis import *\n",
    "from wildfires.dask_cx1 import get_parallel_backend\n",
    "from wildfires.data import *\n",
    "from wildfires.logging_config import enable_logging\n",
    "from wildfires.qstat import get_ncpus\n",
    "from wildfires.utils import *\n",
    "\n",
    "loguru_logger.enable(\"alepython\")\n",
    "loguru_logger.remove()\n",
    "loguru_logger.add(sys.stderr, level=\"WARNING\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "enable_logging(\"jupyter\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Collapsing a non-contiguous coordinate.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*DEFAULT_SPHERICAL_EARTH_RADIUS*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*guessing contiguous bounds*\")\n",
    "\n",
    "normal_coast_linewidth = 0.5\n",
    "mpl.rc(\"figure\", figsize=(14, 6))\n",
    "mpl.rc(\"font\", size=9.0)\n",
    "\n",
    "figure_saver = FigureSaver(\n",
    "    directories=os.path.join(\"~\", \"tmp\", \"analysis_time_lags_explain_pdp_ale\"),\n",
    "    debug=True,\n",
    ")\n",
    "memory = get_memory(\"analysis_time_lags_explain_pdp_ale\", verbose=100)\n",
    "CACHE_DIR = os.path.join(DATA_DIR, \".pickle\", \"time_lags_explain_pdp_ale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrite wildfires get_data with our own personalised version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_time_lag_data import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"symlog\"\n",
    "linthres = 1e-2\n",
    "subs = [2, 3, 4, 5, 6, 7, 8, 9]\n",
    "log_xscale_kwargs = dict(value=value, linthreshx=linthres, subsx=subs)\n",
    "log_yscale_kwargs = dict(value=value, linthreshy=linthres, subsy=subs)\n",
    "log_vars = (\n",
    "    \"dry day period\",\n",
    "    \"popd\",\n",
    "    \"agb tree\",\n",
    "    \"cape x precip\",\n",
    "    \"lai\",\n",
    "    \"shruball\",\n",
    "    \"pftherb\",\n",
    "    \"pftcrop\",\n",
    "    \"treeall\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ale_2d(predictor, train_set, features, bins=40, coverage=1):\n",
    "    if coverage < 1:\n",
    "        # This should be ok if `train_set` is randomised, as it usually is.\n",
    "        train_set = train_set[: int(train_set.shape[0] * coverage)]\n",
    "    ale, quantiles_list, samples_grid = _second_order_ale_quant(\n",
    "        predictor, train_set, features, bins=bins, return_samples_grid=True\n",
    "    )\n",
    "    fig, ax = plt.subplots()\n",
    "    centres_list = [get_centres(quantiles) for quantiles in quantiles_list]\n",
    "    n_x, n_y = 50, 50\n",
    "    x = np.linspace(centres_list[0][0], centres_list[0][-1], n_x)\n",
    "    y = np.linspace(centres_list[1][0], centres_list[1][-1], n_y)\n",
    "\n",
    "    X, Y = np.meshgrid(x, y, indexing=\"xy\")\n",
    "    ale_interp = scipy.interpolate.interp2d(centres_list[0], centres_list[1], ale.T)\n",
    "    CF = ax.contourf(X, Y, ale_interp(x, y), cmap=\"bwr\", levels=30, alpha=0.7)\n",
    "\n",
    "    # Do not autoscale, so that boxes at the edges (contourf only plots the bin\n",
    "    # centres, not their edges) don't enlarge the plot. Such boxes include markings for\n",
    "    # invalid cells, or hatched boxes for valid cells.\n",
    "    plt.autoscale(False)\n",
    "\n",
    "    # Add hatching for the significant cells. These have at least `min_samples` samples.\n",
    "    # By default, calculate this as the number of samples in each bin if everything was equally distributed, divided by 10.\n",
    "    min_samples = (train_set.shape[0] / reduce(mul, map(len, centres_list))) / 10\n",
    "    for i, j in zip(*np.where(samples_grid >= min_samples)):\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                [quantiles_list[0][i], quantiles_list[1][j]],\n",
    "                quantiles_list[0][i + 1] - quantiles_list[0][i],\n",
    "                quantiles_list[1][j + 1] - quantiles_list[1][j],\n",
    "                linewidth=0,\n",
    "                fill=None,\n",
    "                hatch=\".\",\n",
    "                alpha=0.4,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if np.any(ale.mask):\n",
    "        # Add rectangles to indicate cells without samples.\n",
    "        for i, j in zip(*np.where(ale.mask)):\n",
    "            ax.add_patch(\n",
    "                Rectangle(\n",
    "                    [quantiles_list[0][i], quantiles_list[1][j]],\n",
    "                    quantiles_list[0][i + 1] - quantiles_list[0][i],\n",
    "                    quantiles_list[1][j + 1] - quantiles_list[1][j],\n",
    "                    linewidth=1,\n",
    "                    edgecolor=\"k\",\n",
    "                    facecolor=\"none\",\n",
    "                    alpha=0.4,\n",
    "                )\n",
    "            )\n",
    "    fig.colorbar(CF, format=\"%.0e\")\n",
    "    ax.set_xlabel(features[0])\n",
    "    ax.set_ylabel(features[1])\n",
    "    nbins_str = \"x\".join([str(len(centres)) for centres in centres_list])\n",
    "    ax.set_title(\n",
    "        f\"Second-order ALE of features {features[0]} and {features[1]}\\n\"\n",
    "        f\"Bins: {nbins_str} (Hatching: Sig., Boxes: Invalid)\"\n",
    "    )\n",
    "\n",
    "    if any(log_var.lower() in features[0].lower() for log_var in log_vars):\n",
    "        ax.set_xscale(**log_xscale_kwargs)\n",
    "    if any(log_var.lower() in features[1].lower() for log_var in log_vars):\n",
    "        ax.set_yscale(**log_yscale_kwargs)\n",
    "    figure_saver.save_figure(fig, \"__\".join(columns), sub_directory=\"2d_ale_low\")\n",
    "    return ale, quantiles_list, samples_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data Structures used for Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_months = [1, 3, 6, 9, 12, 18, 24]\n",
    "\n",
    "# selection_variables = (\n",
    "#     \"VOD Ku-band -3 Month\",\n",
    "#     # \"SIF\",  # Fix regridding!!\n",
    "#     \"VOD Ku-band -1 Month\",\n",
    "#     \"Dry Day Period -3 Month\",\n",
    "#     \"FAPAR\",\n",
    "#     \"pftHerb\",\n",
    "#     \"LAI -1 Month\",\n",
    "#     \"popd\",\n",
    "#     \"Dry Day Period -24 Month\",\n",
    "#     \"pftCrop\",\n",
    "#     \"FAPAR -1 Month\",\n",
    "#     \"FAPAR -24 Month\",\n",
    "#     \"Max Temp\",\n",
    "#     \"Dry Day Period -6 Month\",\n",
    "#     \"VOD Ku-band -6 Month\",\n",
    "# )\n",
    "\n",
    "# ext_selection_variables = selection_variables + (\n",
    "#     \"Dry Day Period -1 Month\",\n",
    "#     \"FAPAR -6 Month\",\n",
    "#     \"ShrubAll\",\n",
    "#     \"SWI(1)\",\n",
    "#     \"TreeAll\",\n",
    "# )\n",
    "from ipdb import launch_ipdb_on_exception\n",
    "\n",
    "with launch_ipdb_on_exception():\n",
    "    (\n",
    "        e_s_endog_data,\n",
    "        e_s_exog_data,\n",
    "        e_s_master_mask,\n",
    "        e_s_filled_datasets,\n",
    "        e_s_masked_datasets,\n",
    "        e_s_land_mask,\n",
    "    ) = get_data(shift_months=shift_months, selection_variables=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset data from 12 or more months before the current month in order to ease analysis (interpretability).\n",
    "We are interested in the trends in these properties, not their absolute values, therefore we subtract a recent 'seasonal cycle' analogue.\n",
    "This hopefully avoids capturing the same relationships for a variable and its 12 month counterpart due to their high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = []\n",
    "for column in e_s_exog_data:\n",
    "    match = re.search(r\"-\\d{1,2}\", column)\n",
    "    if match:\n",
    "        span = match.span()\n",
    "        # Change the string to reflect the shift.\n",
    "        original_offset = int(column[slice(*span)])\n",
    "        if original_offset > -12:\n",
    "            # Only shift months that are 12 or more months before the current month.\n",
    "            continue\n",
    "        comp = -(-original_offset % 12)\n",
    "        new_column = \" \".join(\n",
    "            (\n",
    "                column[: span[0] - 1],\n",
    "                f\"{original_offset} - {comp}\",\n",
    "                column[span[1] + 1 :],\n",
    "            )\n",
    "        )\n",
    "        if comp == 0:\n",
    "            comp_column = column[: span[0] - 1]\n",
    "        else:\n",
    "            comp_column = \" \".join(\n",
    "                (column[: span[0] - 1], f\"{comp}\", column[span[1] + 1 :])\n",
    "            )\n",
    "        print(column, comp_column)\n",
    "        e_s_exog_data[new_column] = e_s_exog_data[column] - e_s_exog_data[comp_column]\n",
    "        to_delete.append(column)\n",
    "for column in to_delete:\n",
    "    del e_s_exog_data[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cached Model Fitting\n",
    "If anything regarding the data changes above, the cache has to be refreshed using memory.clear()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelResults = namedtuple(\n",
    "    \"ModelResults\",\n",
    "    (\"X_train\", \"X_test\", \"y_train\", \"y_test\", \"r2_test\", \"r2_train\", \"model\"),\n",
    ")\n",
    "\n",
    "model_cache = SimpleCache(\"rf_model\", cache_dir=CACHE_DIR)\n",
    "\n",
    "\n",
    "@model_cache\n",
    "def get_time_lags_model():\n",
    "    \"\"\"Get a RF model trained on the extended shifted data.\n",
    "    \n",
    "    Returns:\n",
    "        ModelResults: A namedtuple with the fields 'X_train', 'X_test', 'y_train', 'y_test', \n",
    "        'r2_test', 'r2_train', and 'model'.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Split the data.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        e_s_exog_data, e_s_endog_data, random_state=1, shuffle=True, test_size=0.3\n",
    "    )\n",
    "    # Define and train the model.\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=None, random_state=1, n_jobs=get_ncpus(),\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    r2_test = rf.score(X_test, y_test)\n",
    "    r2_train = rf.score(X_train, y_train)\n",
    "\n",
    "    return ModelResults(X_train, X_test, y_train, y_test, r2_test, r2_train, rf)\n",
    "\n",
    "\n",
    "model_results = get_time_lags_model()\n",
    "# Take advantage of all cores available to our job.\n",
    "model_results.model.n_jobs = get_ncpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 train:\", model_results.r2_train)\n",
    "print(\"R2 test:\", model_results.r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Feature Importances - Scikit-learn Gini Importances (eli5.explain_weights(_df) does the exact same thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_mean = pd.Series(\n",
    "    model_results.model.feature_importances_, index=model_results.X_train.columns\n",
    ").sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELI5 Permutation Importances (PFI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from wildfires.dask_cx1 import get_parallel_backend\n",
    "\n",
    "perm_importance_cache = SimpleCache(\n",
    "    \"perm_importance\", cache_dir=CACHE_DIR, pickler=cloudpickle\n",
    ")\n",
    "\n",
    "# Does not seem to work with the dask parallel backend - it gets bypassed and every available core on the machine is used up\n",
    "# if attempted.\n",
    "\n",
    "\n",
    "@perm_importance_cache\n",
    "def get_perm_importance():\n",
    "    with parallel_backend(\"threading\", n_jobs=get_ncpus()):\n",
    "        return PermutationImportance(model_results.model).fit(\n",
    "            model_results.X_train, model_results.y_train\n",
    "        )\n",
    "\n",
    "\n",
    "perm_importance = get_perm_importance()\n",
    "perm_df = eli5.explain_weights_df(\n",
    "    perm_importance, feature_names=list(model_results.X_train.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force LOCO (leave one column out) by retraining the model with the relevant column(s) removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.base\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from wildfires.dask_cx1 import *\n",
    "\n",
    "\n",
    "def simple_loco(est, X_train, y_train, leave_out=()):\n",
    "    \"\"\"Simple LOCO feature importances.\n",
    "    \n",
    "    Args:\n",
    "        est: Estimator object with `fit()` and `predict()` methods.\n",
    "        train_X (pandas DataFrame): DataFrame containing the training data.\n",
    "        train_y (pandas Series or array-like): Target data.\n",
    "        leave_out (iterable of column names): Column names to exclude.\n",
    "        \n",
    "    Returns:\n",
    "        mse: Mean squared error of the training set predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get a new instance with the same parameters.\n",
    "    est = sklearn.base.clone(est)\n",
    "\n",
    "    # Fit on the reduced dataset.\n",
    "    X_train = X_train.copy()\n",
    "\n",
    "    for column in leave_out:\n",
    "        del X_train[column]\n",
    "    est.fit(X_train, y_train)\n",
    "    # Get MSE.\n",
    "    mse = mean_squared_error(y_true=y_train, y_pred=est.predict(X_train))\n",
    "    return mse\n",
    "\n",
    "\n",
    "loco_cache = SimpleCache(\"loco_mses\", cache_dir=CACHE_DIR)\n",
    "\n",
    "\n",
    "@loco_cache\n",
    "def get_loco_mses():\n",
    "    # Baseline prediction will be the empty list (first entry here).\n",
    "    leave_out_columns = [[]]\n",
    "    for column in model_results.X_train.columns:\n",
    "        leave_out_columns.append([column])\n",
    "\n",
    "    model_clone = sklearn.base.clone(model_results.model)\n",
    "\n",
    "    with get_parallel_backend(fallback=False):\n",
    "        mse_values = Parallel(verbose=10)(\n",
    "            delayed(simple_loco)(\n",
    "                model_clone, model_results.X_train, model_results.y_train, columns\n",
    "            )\n",
    "            for columns in tqdm(leave_out_columns, desc=\"Prefetch LOCO columns\")\n",
    "        )\n",
    "    return leave_out_columns, mse_values\n",
    "\n",
    "\n",
    "leave_out_columns, mse_values = get_loco_mses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "mse_values = np.asarray(mse_values)\n",
    "assert leave_out_columns[0] == []\n",
    "loco_columns = [\"baseline\"] + [\"_\".join(columns) for columns in leave_out_columns[1:]]\n",
    "baseline_mse = mse_values[0]\n",
    "loco_importances = pd.Series(\n",
    "    mse_values[1:] - baseline_mse, index=loco_columns[1:]\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "if np.any(loco_importances < 0):\n",
    "    warn(\"MSE values without some features were lower than baseline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the three measures - Gini vs PFI vs LOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_import_df = pd.DataFrame(\n",
    "    np.hstack(\n",
    "        (\n",
    "            gini_mean.index.values[:, np.newaxis],\n",
    "            gini_mean.values[:, np.newaxis],\n",
    "            perm_df[\"feature\"].values[:, np.newaxis],\n",
    "            perm_df[\"weight\"].values[:, np.newaxis],\n",
    "            loco_importances.index.values[:, np.newaxis],\n",
    "            loco_importances.values[:, np.newaxis],\n",
    "        )\n",
    "    ),\n",
    "    columns=[[\"Gini\"] * 2 + [\"PFI\"] * 2 + [\"LOCO\"] * 2, [\"Feature\", \"Importance\"] * 3],\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(23, 15))\n",
    "for ax, measure in zip(axes, (\"Gini\", \"PFI\", \"LOCO\")):\n",
    "    features = list(comp_import_df[(measure, \"Feature\")])[::-1]\n",
    "    importances = np.asarray(comp_import_df[(measure, \"Importance\")])[::-1]\n",
    "    importances /= np.sum(importances)\n",
    "    ax.set_title(measure)\n",
    "    ax.barh(\n",
    "        range(len(features)),\n",
    "        importances,\n",
    "        align=\"center\",\n",
    "        color=sns.color_palette(\"husl\", len(features), desat=0.5)[::-1],\n",
    "    )\n",
    "    ax.set_yticks(range(len(features)))\n",
    "    ax.set_yticklabels(features)\n",
    "    ax.set_xlabel(f\"Relative {measure} Importance\")\n",
    "    ax.margins(y=0.008, tight=True)\n",
    "plt.subplots_adjust(wspace=0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Tree Importances - Gini vs PFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(2, 1, sharex=True, figsize=(28, 14))\n",
    "\n",
    "# Gini values.\n",
    "ind_trees_gini = pd.DataFrame(\n",
    "    [tree.feature_importances_ for tree in model_results.model],\n",
    "    columns=model_results.X_train.columns,\n",
    ")\n",
    "mean_importances = ind_trees_gini.mean().sort_values(ascending=False)\n",
    "ind_trees_gini = ind_trees_gini.reindex(mean_importances.index, axis=1)\n",
    "sns.boxplot(data=ind_trees_gini, ax=ax)\n",
    "ax.set(title=\"Gini Importances\", ylabel=\"Gini Importance (MSE)\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "# PFI values.\n",
    "pfi_ind = pd.DataFrame(perm_importance.results_, columns=model_results.X_train.columns)\n",
    "\n",
    "# Re-index according to the same ordering as for the Gini importances!\n",
    "pfi_ind = pfi_ind.reindex(mean_importances.index, axis=1)\n",
    "\n",
    "sns.boxplot(data=pfi_ind, ax=ax2)\n",
    "ax2.set(title=\"PFI Importances\", ylabel=\"PFI Importance\")\n",
    "_ = ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "for _ax in (ax, ax2):\n",
    "    _ax.grid(which=\"major\", alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "class MidpointNormalize(colors.Normalize):\n",
    "    def __init__(self, *args, midpoint=None, **kwargs):\n",
    "        self.midpoint = midpoint\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        # Simple mapping between the color range halves and the data halves.\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "\n",
    "def corr_plot(exog_data):\n",
    "    columns = list(map(map_name, exog_data.columns))\n",
    "\n",
    "    def trim(string, n=10, cont_str=\"...\"):\n",
    "        if len(string) > n:\n",
    "            string = string[: n - len(cont_str)]\n",
    "            string += cont_str\n",
    "        return string\n",
    "\n",
    "    n = len(columns)\n",
    "    fig, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "    corr_arr = np.ma.MaskedArray(exog_data.corr().values)\n",
    "    corr_arr.mask = np.zeros_like(corr_arr)\n",
    "    # Ignore diagnals, since they will all be 1 anyway!\n",
    "    np.fill_diagonal(corr_arr.mask, True)\n",
    "\n",
    "    im = ax.matshow(\n",
    "        corr_arr,\n",
    "        interpolation=\"none\",\n",
    "        cmap=\"RdYlBu_r\",\n",
    "        norm=MidpointNormalize(midpoint=0.0),\n",
    "    )\n",
    "\n",
    "    fig.colorbar(im, pad=0.03, shrink=0.95, aspect=20)\n",
    "\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_xticklabels(map(partial(trim, n=15), columns))\n",
    "    ax.set_yticks(np.arange(n))\n",
    "    ax.set_yticklabels(columns)\n",
    "\n",
    "    # Activate ticks on top of axes.\n",
    "    ax.tick_params(axis=\"x\", bottom=False, top=True, labelbottom=False, labeltop=True)\n",
    "\n",
    "    # Rotate and align top ticklabels\n",
    "    plt.setp(\n",
    "        [tick.label2 for tick in ax.xaxis.get_major_ticks()],\n",
    "        rotation=45,\n",
    "        ha=\"left\",\n",
    "        va=\"center\",\n",
    "        rotation_mode=\"anchor\",\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "corr_plot(model_results.X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDP Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdpbox import info_plots, pdp\n",
    "\n",
    "model_results.model.n_jobs = get_ncpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_dry_day_period = pdp.pdp_isolate(\n",
    "    model=model_results.model,\n",
    "    dataset=model_results.X_train[:10000],\n",
    "    model_features=model_results.X_train.columns,\n",
    "    feature=\"Dry Day Period\",\n",
    ")\n",
    "fig, axes = pdp.pdp_plot(\n",
    "    pdp_dry_day_period,\n",
    "    \"Dry Day Period\",\n",
    "    plot_lines=True,\n",
    "    frac_to_plot=1,\n",
    "    x_quantile=True,\n",
    "    cluster=False,\n",
    "    n_cluster_centers=20,\n",
    "    show_percentile=True,\n",
    "    plot_pts_dist=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = pdp.pdp_plot(\n",
    "    pdp_dry_day_period,\n",
    "    \"Dry Day Period\",\n",
    "    plot_lines=True,\n",
    "    frac_to_plot=1,\n",
    "    x_quantile=True,\n",
    "    cluster=True,\n",
    "    n_cluster_centers=40,\n",
    "    show_percentile=True,\n",
    "    plot_pts_dist=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_fapar_dry_day = pdp.pdp_interact(\n",
    "    model=model_results.model,\n",
    "    dataset=model_results.X_train,\n",
    "    features=[\"FAPAR\", \"Dry Day Period\"],\n",
    "    num_grid_points=(10, 10),\n",
    "    percentile_ranges=[(5, 95), (5, 95)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worldwide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ale_plot_1d(model, X_train, column):\n",
    "    with figure_saver(column, sub_directory=\"ale\"):\n",
    "        ale_plot(\n",
    "            model,\n",
    "            X_train,\n",
    "            column,\n",
    "            bins=40,\n",
    "            monte_carlo=False,  # XXX: !!!\n",
    "            monte_carlo_rep=100,\n",
    "            monte_carlo_ratio=0.01,\n",
    "            plot_quantiles=False,\n",
    "        )\n",
    "        plt.gcf().axes[0].lines[-1].set_marker(\".\")\n",
    "        if any(feature.lower() in column.lower() for feature in log_vars):\n",
    "            plt.gcf().axes[0].set_xscale(**log_xscale_kwargs)\n",
    "\n",
    "\n",
    "target_func = save_ale_plot_1d\n",
    "\n",
    "model_params = (model_results.model, model_results.X_train[:100])\n",
    "\n",
    "with get_parallel_backend(fallback=\"none\") as (backend, client):\n",
    "    if client is not None:\n",
    "        print(\"Using Dask\", client)\n",
    "        # A Dask scheduler was found, so we need to scatter large pieces of data (if any).\n",
    "\n",
    "        model_params = [client.scatter(param, broadcast=True) for param in model_params]\n",
    "\n",
    "        def func(param_iter):\n",
    "            return client.gather(client.map(target_func, *list(zip(*(param_iter)))))\n",
    "\n",
    "    else:\n",
    "        print(\"Not using any backend\")\n",
    "\n",
    "        def func(param_iter):\n",
    "            return [target_func(*params) for params in param_iter]\n",
    "\n",
    "    func(\n",
    "        (*model_params, column)\n",
    "        for column in tqdm(model_results.X_train.columns, desc=\"ALE plotting\")\n",
    "        if column == \"lightning\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D ALE interaction plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = 0.02\n",
    "\n",
    "\n",
    "def plot_ale_and_get_importance(columns, model, train_set):\n",
    "    model.n_jobs = get_ncpus()\n",
    "    ale, quantiles_list, samples_grid = ale_2d(\n",
    "        model.predict, train_set, columns, bins=20, coverage=coverage,\n",
    "    )\n",
    "    min_samples = (\n",
    "        train_set.shape[0] / reduce(mul, map(lambda x: len(x) - 1, quantiles_list))\n",
    "    ) / 10\n",
    "    try:\n",
    "        return np.ma.max(ale[samples_grid > min_samples]) - np.ma.min(\n",
    "            ale[samples_grid > min_samples]\n",
    "        )\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp_values = {}\n",
    "columns_list = list(combinations(model_results.X_train.columns, 2))\n",
    "for columns in tqdm(columns_list, desc=\"Calculating 2D ALE plots\"):\n",
    "    ptp_values[columns] = plot_ale_and_get_importance(\n",
    "        columns, model_results.model, model_results.X_train\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore and count None values, then plot a histogram of the ptp values.\n",
    "filtered_columns_list = []\n",
    "filtered_ptp_values = []\n",
    "for columns, ptp in ptp_values.items():\n",
    "    if ptp is not None:\n",
    "        filtered_columns_list.append(columns)\n",
    "        filtered_ptp_values.append(ptp)\n",
    "\n",
    "np.asarray([ptp for ptp in ptp_values if ptp is not None])\n",
    "_ = plt.hist(filtered_ptp_values, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_results = pd.Series(filtered_ptp_values, index=filtered_columns_list)\n",
    "pdp_results.sort_values(inplace=True, ascending=False)\n",
    "print(pdp_results.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the original DataFrame to analyse specific regions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataframe(data, original_mask, additional_mask, suffix=\"\"):\n",
    "    \"\"\"Sub-set results based on an additional mask.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.core.frame.DataFrame): Data to select.\n",
    "        orig_mask (array-like): Original mask that was used to transform the data into the column representation in `data`. This mask should be False where data should be selected.\n",
    "        additional_mask (array-like): After conversion of columns in `data` back to a lat-lon grid, this mask will be used in addition to `orig_mask` to return a subset of the data to the column format. This mask should be False where data should be selected.\n",
    "        suffix (str): Suffix to add to column labels. An empty space will be added to the beginning of `suffix` if this is not already present.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.core.frame.DataFrame: Selected data.\n",
    "    \n",
    "    \"\"\"\n",
    "    additional_mask = match_shape(additional_mask, original_mask.shape)\n",
    "    if suffix:\n",
    "        if suffix[0] != \" \":\n",
    "            suffix = \" \" + suffix\n",
    "    new_data = {}\n",
    "    for column in tqdm(data.columns, desc=\"Selecting data\"):\n",
    "        # Create a blank lat-lon grid.\n",
    "        lat_lon_data = np.empty_like(original_mask, dtype=np.float64)\n",
    "        # Convert data from the dense column representation to the sparse lat-lon grid.\n",
    "        lat_lon_data[~original_mask] = data[column]\n",
    "        # Use the original and the new mask to create new columns.\n",
    "        new_data[column + suffix] = lat_lon_data[\n",
    "            ((~original_mask) & (~additional_mask))\n",
    "        ]\n",
    "    return pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SE Asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new mask.\n",
    "region_mask = ~box_mask(lats=(-10, 10), lons=(95, 150))\n",
    "cube_plotting(region_mask)\n",
    "\n",
    "# XXX: This only allows subsetting the original data (both training and test!) since otherwise the original mask does not apply.\n",
    "\n",
    "# Apply new mask.\n",
    "sub_X = subset_dataframe(e_s_exog_data, e_s_master_mask, region_mask, \"SE ASIA\")\n",
    "print(\"Original size:\", e_s_exog_data.shape)\n",
    "print(\"Selected size:\", sub_X.shape)\n",
    "\n",
    "# Plot ALE plots for only this region.\n",
    "for column in tqdm(sub_X.columns, desc=\"Calculating ALE plots\"):\n",
    "    with figure_saver(column, sub_directory=\"ale_se_asia\"):\n",
    "        ale_plot(\n",
    "            model_results.model,\n",
    "            sub_X,\n",
    "            column,\n",
    "            bins=40,\n",
    "            monte_carlo=True,\n",
    "            monte_carlo_rep=30,\n",
    "            monte_carlo_ratio=0.1,\n",
    "            verbose=False,\n",
    "            log=\"x\"\n",
    "            if any(\n",
    "                feature.lower() in column.lower()\n",
    "                for feature in (\"dry day period\", \"popd\",)\n",
    "            )\n",
    "            else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brazilian Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new mask.\n",
    "region_mask = ~box_mask(lats=(-15, 1), lons=(-72, -46))\n",
    "cube_plotting(region_mask)\n",
    "\n",
    "# XXX: This only allows subsetting the original data (both training and test!) since otherwise the original mask does not apply.\n",
    "\n",
    "# Apply new mask.\n",
    "sub_X = subset_dataframe(e_s_exog_data, e_s_master_mask, region_mask, \"BRAZ AMAZ\")\n",
    "print(\"Original size:\", e_s_exog_data.shape)\n",
    "print(\"Selected size:\", sub_X.shape)\n",
    "\n",
    "# Plot ALE plots for only this region.\n",
    "for column in tqdm(sub_X.columns, desc=\"Calculating ALE plots\"):\n",
    "    with figure_saver(column, sub_directory=\"ale_braz_amaz\"):\n",
    "        ale_plot(\n",
    "            model_results.model,\n",
    "            sub_X,\n",
    "            column,\n",
    "            bins=40,\n",
    "            monte_carlo=True,\n",
    "            monte_carlo_rep=30,\n",
    "            monte_carlo_ratio=0.1,\n",
    "            verbose=False,\n",
    "            log=\"x\"\n",
    "            if any(\n",
    "                feature.lower() in column.lower()\n",
    "                for feature in (\"dry day period\", \"popd\",)\n",
    "            )\n",
    "            else None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new mask.\n",
    "region_mask = ~box_mask(lats=(33, 73), lons=(-11, 29))\n",
    "cube_plotting(region_mask)\n",
    "\n",
    "# XXX: This only allows subsetting the original data (both training and test!) since otherwise the original mask does not apply.\n",
    "\n",
    "# Apply new mask.\n",
    "sub_X = subset_dataframe(e_s_exog_data, e_s_master_mask, region_mask, \"EUROPE\")\n",
    "print(\"Original size:\", e_s_exog_data.shape)\n",
    "print(\"Selected size:\", sub_X.shape)\n",
    "\n",
    "# Plot ALE plots for only this region.\n",
    "for column in tqdm(sub_X.columns, desc=\"Calculating ALE plots\"):\n",
    "    with figure_saver(column, sub_directory=\"ale_europe\"):\n",
    "        ale_plot(\n",
    "            model_results.model,\n",
    "            sub_X,\n",
    "            column,\n",
    "            bins=40,\n",
    "            monte_carlo=True,\n",
    "            monte_carlo_rep=30,\n",
    "            monte_carlo_ratio=0.1,\n",
    "            verbose=False,\n",
    "            log=\"x\"\n",
    "            if any(\n",
    "                feature.lower() in column.lower()\n",
    "                for feature in (\"dry day period\", \"popd\",)\n",
    "            )\n",
    "            else None,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
