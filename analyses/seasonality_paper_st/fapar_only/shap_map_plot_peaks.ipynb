{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display_html\n",
    "from specific import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    endog_data,\n",
    "    exog_data,\n",
    "    master_mask,\n",
    "    filled_datasets,\n",
    "    masked_datasets,\n",
    "    land_mask,\n",
    ") = get_offset_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve previous results from the 'model' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_split_cache.load()\n",
    "rf = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap_cache.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate 2D masked array SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_shap_results = calculate_2d_masked_shap_values(X_train, master_mask, shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ba = get_masked_array(endog_data.values, master_mask)\n",
    "mean_ba = np.ma.mean(target_ba, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use significant peak finding algorithm to determine mean timing of maximum impact using SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_threshold = 0.5\n",
    "ptp_threshold_factor = 0.12  # relative to the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot weighted SHAP peak locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create combined figure of weighted SHAP values for a given set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_month = 9\n",
    "\n",
    "\n",
    "def param_iter():\n",
    "    for feature_name in tqdm([\"FAPAR\", \"Dry Day Period\"], desc=\"Feature\"):\n",
    "        for exclude_inst in tqdm([False, True], desc=\"Exclude inst.\"):\n",
    "            yield exclude_inst, feature_name\n",
    "\n",
    "\n",
    "filter_name = \"normal\"\n",
    "shap_results = map_shap_results\n",
    "shap_measure = \"masked_max_shap_arrs\"\n",
    "\n",
    "weighted_plot_cache = SimpleCache(\n",
    "    f\"weighted_plot_{filter_name}_{shap_measure}\", cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "\n",
    "@weighted_plot_cache\n",
    "def get_weighted_plot_data():\n",
    "    plot_data = {}\n",
    "    for exclude_inst, feature_name in param_iter():\n",
    "        short_feature = shorten_features(feature_name)\n",
    "\n",
    "        filtered = np.array(filter_by_month(X_train.columns, feature_name, max_month))\n",
    "        lags = np.array(\n",
    "            [get_lag(feature, target_feature=feature_name) for feature in filtered]\n",
    "        )\n",
    "\n",
    "        # Ensure lags are sorted consistently.\n",
    "        lag_sort_inds = np.argsort(lags)\n",
    "        filtered = tuple(filtered[lag_sort_inds])\n",
    "        lags = tuple(lags[lag_sort_inds])\n",
    "\n",
    "        if exclude_inst and 0 in lags:\n",
    "            assert lags[0] == 0\n",
    "            lags = lags[1:]\n",
    "            filtered = filtered[1:]\n",
    "\n",
    "        n_features = len(filtered)\n",
    "\n",
    "        # There is no point plotting this map for a single feature or less since we are\n",
    "        # interested in a comparison between different feature ranks.\n",
    "        if n_features <= 1:\n",
    "            continue\n",
    "\n",
    "        selected_data = np.empty(n_features, dtype=object)\n",
    "        for i, col in enumerate(X_train.columns):\n",
    "            if col in filtered:\n",
    "                selected_data[lags.index(get_lag(col))] = shap_results[shap_measure][\n",
    "                    \"data\"\n",
    "                ][i].copy()\n",
    "\n",
    "        shared_mask = reduce(np.logical_or, (data.mask for data in selected_data))\n",
    "        for data in selected_data:\n",
    "            data.mask = shared_mask\n",
    "\n",
    "        stacked_shaps = np.vstack([data.data[np.newaxis] for data in selected_data])\n",
    "\n",
    "        # Calculate the significance of the global maxima for each of the valid pixels.\n",
    "\n",
    "        # Valid indices are recorded in 'shared_mask'.\n",
    "\n",
    "        valid_i, valid_j = np.where(~shared_mask)\n",
    "        total_valid = len(valid_i)\n",
    "\n",
    "        max_positions = np.ma.MaskedArray(\n",
    "            np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "        )\n",
    "        for i, j in zip(tqdm(valid_i, desc=\"Evaluating maxima\", smoothing=0), valid_j):\n",
    "            ptp_threshold = ptp_threshold_factor * mean_ba[i, j]\n",
    "            if significant_peak(\n",
    "                stacked_shaps[:, i, j],\n",
    "                diff_threshold=diff_threshold,\n",
    "                ptp_threshold=ptp_threshold,\n",
    "            ):\n",
    "                # If the maximum is significant, go on the calculate the weighted avg. of the signal.\n",
    "                max_positions[i, j] = np.average(\n",
    "                    lags, weights=np.abs(stacked_shaps[:, i, j])\n",
    "                )\n",
    "\n",
    "        plot_data[(exclude_inst, short_feature)] = max_positions\n",
    "\n",
    "    return plot_data\n",
    "\n",
    "\n",
    "weighted_plot_data = get_weighted_plot_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(9, 5))\n",
    "for (ax, ((exclude_inst, short_feature), max_positions)) in zip(\n",
    "    axes.flatten(), weighted_plot_data.items()\n",
    "):\n",
    "    ax.hist(get_unmasked(max_positions.flatten()), bins=\"auto\", density=True)\n",
    "    ax.set_title(f\"Feature: {short_feature} - Excl. Inst: {exclude_inst}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orientation = \"horizontal\"\n",
    "\n",
    "with SetupFourMapAxes(cbar=orientation) as (fig, axes, cax):\n",
    "    for (i, (ax, ((exclude_inst, short_feature), max_positions), title)) in enumerate(\n",
    "        zip(\n",
    "            axes,\n",
    "            weighted_plot_data.items(),\n",
    "            ascii_lowercase[: len(axes)],\n",
    "        )\n",
    "    ):\n",
    "        cube_plotting(\n",
    "            max_positions,\n",
    "            title=\"\",\n",
    "            colorbar_kwargs=False\n",
    "            if i < 3\n",
    "            else dict(\n",
    "                label=\"month\",\n",
    "                format=\"%0.0f\",\n",
    "                cax=cax,\n",
    "                orientation=orientation,\n",
    "            ),\n",
    "            coastline_kwargs={\"linewidth\": 0.3},\n",
    "            vmin=1,\n",
    "            vmax=6,\n",
    "            ax=ax,\n",
    "        )\n",
    "        if exclude_inst:\n",
    "            exc_string = f\"(no current {short_feature})\"\n",
    "        else:\n",
    "            exc_string = f\"(with current {short_feature})\"\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            1.03,\n",
    "            f\"({title}) {short_feature} {exc_string}\",\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"center\",\n",
    "        )\n",
    "\n",
    "# Save the combined figure.\n",
    "map_figure_saver.save_figure(\n",
    "    fig,\n",
    "    \"normal_ba_weighted_max_shap_fapar_dry_days\",\n",
    "    sub_directory=Path(\"weighted_shap_maps\"),\n",
    "    dpi=350,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorisation into multiple peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfts = ESA_CCI_Landcover_PFT()\n",
    "pfts.limit_months(start=PartialDateTime(2010, 1), end=PartialDateTime(2015, 1))\n",
    "pfts.regrid()\n",
    "pfts = pfts.get_mean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_month = 9\n",
    "close_figs = True\n",
    "verbose = 2\n",
    "\n",
    "mpl.rc(\"grid\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "\n",
    "def param_iter():\n",
    "    for feature_name in tqdm(\n",
    "        [\"FAPAR\", \"Dry Day Period\"], desc=\"Feature\", disable=verbose < 1\n",
    "    ):\n",
    "        for (exc_name, exclude_inst) in tqdm(\n",
    "            [(\"with_inst\", False), (\"no_inst\", True)],\n",
    "            desc=\"Exclude inst.\",\n",
    "            disable=verbose < 2,\n",
    "        ):\n",
    "            yield (exc_name, exclude_inst), feature_name\n",
    "\n",
    "\n",
    "filter_name = \"normal\"\n",
    "shap_results = map_shap_results\n",
    "shap_measure = \"masked_max_shap_arrs\"\n",
    "\n",
    "peak_cache = SimpleCache(f\"peak_data_{filter_name}_{shap_measure}\", cache_dir=CACHE_DIR)\n",
    "\n",
    "\n",
    "@peak_cache\n",
    "def get_data():\n",
    "    data_dict = {}\n",
    "    for ((exc_name, exclude_inst), feature_name) in param_iter():\n",
    "        short_feature = shorten_features(feature_name)\n",
    "\n",
    "        filtered = np.array(filter_by_month(X_train.columns, feature_name, max_month))\n",
    "        lags = np.array(\n",
    "            [get_lag(feature, target_feature=feature_name) for feature in filtered]\n",
    "        )\n",
    "\n",
    "        # Ensure lags are sorted consistently.\n",
    "        lag_sort_inds = np.argsort(lags)\n",
    "        filtered = tuple(filtered[lag_sort_inds])\n",
    "        lags = tuple(lags[lag_sort_inds])\n",
    "\n",
    "        if exclude_inst and 0 in lags:\n",
    "            assert lags[0] == 0\n",
    "            lags = lags[1:]\n",
    "            filtered = filtered[1:]\n",
    "\n",
    "        n_features = len(filtered)\n",
    "\n",
    "        # There is no point plotting this map for a single feature or less since we are\n",
    "        # interested in a comparison between different feature ranks.\n",
    "        if n_features <= 1:\n",
    "            continue\n",
    "\n",
    "        selected_data = np.empty(n_features, dtype=object)\n",
    "        for i, col in enumerate(X_train.columns):\n",
    "            if col in filtered:\n",
    "                selected_data[lags.index(get_lag(col))] = shap_results[shap_measure][\n",
    "                    \"data\"\n",
    "                ][i].copy()\n",
    "\n",
    "        shared_mask = reduce(np.logical_or, (data.mask for data in selected_data))\n",
    "        for data in selected_data:\n",
    "            data.mask = shared_mask\n",
    "\n",
    "        stacked_shaps = np.vstack([data.data[np.newaxis] for data in selected_data])\n",
    "\n",
    "        # Calculate the significance of the global maxima for each of the valid pixels.\n",
    "\n",
    "        # Valid indices are recorded in 'shared_mask'.\n",
    "\n",
    "        valid_i, valid_j = np.where(~shared_mask)\n",
    "        total_valid = len(valid_i)\n",
    "\n",
    "        max_positions = np.ma.MaskedArray(\n",
    "            np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "        )\n",
    "\n",
    "        peak_indices = []\n",
    "\n",
    "        for i, j in zip(\n",
    "            tqdm(valid_i, desc=\"Evaluating maxima\", smoothing=0, disable=verbose < 3),\n",
    "            valid_j,\n",
    "        ):\n",
    "            ptp_threshold = ptp_threshold_factor * mean_ba[i, j]\n",
    "            peaks_i = significant_peak(\n",
    "                stacked_shaps[:, i, j],\n",
    "                diff_threshold=diff_threshold,\n",
    "                ptp_threshold=ptp_threshold,\n",
    "                strict=False,\n",
    "            )\n",
    "\n",
    "            # Disregarding the sign of the mean influence, sorted by absolute\n",
    "            # value (not peak height) magnitude.\n",
    "            #\n",
    "            # peak_indices.append(peaks_i)\n",
    "\n",
    "            # Adding information about the sign of the mean influence, sorted by absolute\n",
    "            # value (not peak height) magnitude.\n",
    "            #\n",
    "            #         peak_indices.append(tuple(\n",
    "            #             f\"{p_i}({'+' if stacked_shaps[p_i, i, j] > 0 else '-'})\" for p_i in peaks_i\n",
    "            #         ))\n",
    "\n",
    "            # Adding information about the sign of the mean influence, sorted by time.\n",
    "            #\n",
    "            peak_indices.append(\n",
    "                tuple(\n",
    "                    f\"{lags[p_i]}({'+' if stacked_shaps[p_i, i, j] > 0 else '-'})\"\n",
    "                    for p_i in sorted(peaks_i)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        data_dict[(exclude_inst, feature_name)] = (\n",
    "            filtered,\n",
    "            lags,\n",
    "            n_features,\n",
    "            valid_i,\n",
    "            valid_j,\n",
    "            total_valid,\n",
    "            peak_indices,\n",
    "            shared_mask,\n",
    "        )\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "peak_data_dict = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _peak_data_iter():\n",
    "    for ((exc_name, exclude_inst), feature_name) in islice(param_iter(), 0, None):\n",
    "        short_feature = shorten_features(feature_name)\n",
    "\n",
    "        sub_directory = (\n",
    "            Path(\"shap_peaks\") / filter_name / shap_measure / short_feature / exc_name\n",
    "        )\n",
    "\n",
    "        (\n",
    "            filtered,\n",
    "            lags,\n",
    "            n_features,\n",
    "            valid_i,\n",
    "            valid_j,\n",
    "            total_valid,\n",
    "            peak_indices,\n",
    "            shared_mask,\n",
    "        ) = peak_data_dict[(exclude_inst, feature_name)]\n",
    "\n",
    "        # Determine the number of peaks at each location.\n",
    "        peaks_arr = np.ma.MaskedArray(\n",
    "            np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "        )\n",
    "        for i, j, indices in zip(valid_i, valid_j, peak_indices):\n",
    "            peaks_arr[i, j] = len(indices)\n",
    "\n",
    "        # Exclude locations with too many peaks.\n",
    "        masked_peaks = peaks_arr.copy()\n",
    "        masked_peaks.mask |= (peaks_arr.data == 0) | (peaks_arr.data > 2)\n",
    "\n",
    "        # Determine the peaks present at each valid location.\n",
    "        valid_peak_indices = []\n",
    "        for i, j, peaks_i in zip(valid_i, valid_j, peak_indices):\n",
    "            if masked_peaks.mask[i, j]:\n",
    "                # Only use valid samples.\n",
    "                continue\n",
    "            valid_peak_indices.append(peaks_i)\n",
    "\n",
    "        assert np.all(\n",
    "            np.sort(np.unique([len(indices) for indices in valid_peak_indices]))\n",
    "            == np.array([1, 2])\n",
    "        )\n",
    "\n",
    "        peaks_dict = dict(zip(*np.unique(valid_peak_indices, return_counts=True)))\n",
    "\n",
    "        total_counts = np.sum(list(peaks_dict.values()))\n",
    "        relative_counts_dict = {\n",
    "            key: val / total_counts for key, val in peaks_dict.items()\n",
    "        }\n",
    "\n",
    "        #\n",
    "        # Limit the number of peak combinations.\n",
    "        #\n",
    "\n",
    "        keys, values = list(zip(*relative_counts_dict.items()))\n",
    "        keys = np.asarray(keys)\n",
    "        values = np.asarray(values)\n",
    "\n",
    "        max_n_peaks = 6\n",
    "        min_frac = 0.075  # Require at least this fraction per entry. (0.05)\n",
    "\n",
    "        sorted_indices = np.argsort(values)\n",
    "        cumulative_fractions = np.cumsum(values[sorted_indices])\n",
    "\n",
    "        # Ensure at least `min_n_entries` entries are present, but no more than `max_n_peaks`.\n",
    "        mask = np.ones_like(cumulative_fractions, dtype=np.bool_)\n",
    "        mask[:-max_n_peaks] = False  # no more than `max_n_peaks`.\n",
    "\n",
    "        mask &= values[sorted_indices] > min_frac\n",
    "\n",
    "        print(\n",
    "            f\"Remaining fraction: {short_feature}, exclude inst: {exclude_inst}\",\n",
    "            np.sum(values[sorted_indices][mask]),\n",
    "        )\n",
    "\n",
    "        thres_counts_dict = {\n",
    "            key: val\n",
    "            for key, val in zip(\n",
    "                keys[sorted_indices][mask], values[sorted_indices][mask]\n",
    "            )\n",
    "        }\n",
    "        print(f\"{short_feature}, exclude inst: {exclude_inst}\", thres_counts_dict)\n",
    "\n",
    "        peak_keys = list(thres_counts_dict)\n",
    "\n",
    "        imp_peaks = np.ma.MaskedArray(\n",
    "            np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "        )\n",
    "        for i, j, indices in zip(valid_i, valid_j, peak_indices):\n",
    "            if indices in peak_keys:\n",
    "                imp_peaks[i, j] = peak_keys.index(indices)\n",
    "\n",
    "        assert len(np.unique(imp_peaks.data[~imp_peaks.mask])) == len(peak_keys)\n",
    "\n",
    "        peak_results = {}\n",
    "        for comb_i in tqdm(np.unique(get_unmasked(imp_peaks)), desc=\"Peak combination\"):\n",
    "            for pft_cube in pfts:\n",
    "                selection = (~(pft_cube.data.mask | imp_peaks.mask)) & np.isclose(\n",
    "                    imp_peaks, comb_i\n",
    "                )\n",
    "                peak_results[\n",
    "                    (str(peak_keys[int(comb_i)]), pft_cube.name())\n",
    "                ] = pft_cube.data.data[selection]\n",
    "\n",
    "        peak_comb_df = pd.DataFrame(\n",
    "            {key: pd.Series(vals) for key, vals in peak_results.items()}\n",
    "        )\n",
    "        peak_comb_df.columns.names = [\"peak_combination\", \"pft\"]\n",
    "\n",
    "        yield dict(\n",
    "            exc_name=exc_name,\n",
    "            exclude_inst=exclude_inst,\n",
    "            short_feature=short_feature,\n",
    "            sub_directory=sub_directory,\n",
    "            filtered=filtered,\n",
    "            lags=lags,\n",
    "            n_features=n_features,\n",
    "            valid_i=valid_i,\n",
    "            valid_j=valid_j,\n",
    "            total_valid=total_valid,\n",
    "            peak_indices=peak_indices,\n",
    "            peaks_arr=peaks_arr,\n",
    "            masked_peaks=masked_peaks,\n",
    "            valid_peak_indices=valid_peak_indices,\n",
    "            peaks_dict=peaks_dict,\n",
    "            total_counts=total_counts,\n",
    "            relative_counts_dict=relative_counts_dict,\n",
    "            thres_counts_dict=thres_counts_dict,\n",
    "            peak_keys=peak_keys,\n",
    "            imp_peaks=imp_peaks,\n",
    "            peak_comb_df=peak_comb_df,\n",
    "        )\n",
    "\n",
    "\n",
    "peak_data_dicts = list(_peak_data_iter())\n",
    "\n",
    "\n",
    "def peak_data_iter():\n",
    "    for data_dict in peak_data_dicts:\n",
    "        yield data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the number of peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "\n",
    "    pd.Series(\n",
    "        dict(\n",
    "            zip(\n",
    "                *np.unique(\n",
    "                    [len(indices) for indices in plot_data[\"peak_indices\"]],\n",
    "                    return_counts=True,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ).plot.bar(\n",
    "        ax=plt.subplots(figsize=(6, 4))[1],\n",
    "        title=f\"{short_feature}, {exc_name}\",\n",
    "        rot=0,\n",
    "    )\n",
    "    figure_saver.save_figure(plt.gcf(), \"n_peaks_distr\", sub_directory=sub_directory)\n",
    "    if close_figs:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of the number of peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "\n",
    "    fig, cbar = cube_plotting(\n",
    "        plot_data[\"peaks_arr\"],\n",
    "        title=f\"Nr. Peaks {short_feature}, {exc_name}\",\n",
    "        boundaries=np.arange(0, 4) - 0.5,\n",
    "        fig=plt.figure(figsize=(5.1, 2.6)),\n",
    "        coastline_kwargs={\"linewidth\": 0.3},\n",
    "        colorbar_kwargs={\"label\": \"nr. peaks\", \"format\": \"%0.1f\"},\n",
    "        return_cbar=True,\n",
    "    )\n",
    "\n",
    "    tick_pos = np.arange(4, dtype=np.float64)\n",
    "    tick_pos[3] -= 0.5\n",
    "\n",
    "    cbar.set_ticks(tick_pos)\n",
    "\n",
    "    tick_labels = list(map(str, range(3))) + [\">2\"]\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    map_figure_saver.save_figure(\n",
    "        fig, f\"nr_shap_peaks_map_{short_feature}\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nr. peaks map with large nr. peaks filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "\n",
    "    cmap, norm = from_levels_and_colors(\n",
    "        levels=np.arange(1, 4) - 0.5,\n",
    "        colors=[\"C1\", \"C2\"],\n",
    "        extend=\"neither\",\n",
    "    )\n",
    "\n",
    "    fig, cbar = cube_plotting(\n",
    "        plot_data[\"masked_peaks\"],\n",
    "        title=f\"Nr. Peaks {short_feature}, {exc_name}\",\n",
    "        # boundaries=np.arange(1, 4) - 0.5,\n",
    "        fig=plt.figure(figsize=(5.1, 2.6)),\n",
    "        coastline_kwargs={\"linewidth\": 0.3},\n",
    "        colorbar_kwargs={\"label\": \"nr. peaks\", \"format\": \"%0.1f\"},\n",
    "        return_cbar=True,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "    )\n",
    "\n",
    "    tick_pos = np.arange(3, dtype=np.float64)\n",
    "    cbar.set_ticks(tick_pos)\n",
    "\n",
    "    tick_labels = list(map(str, range(1, 3)))\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    #     plt.gca().gridlines()\n",
    "\n",
    "    map_figure_saver.save_figure(\n",
    "        fig, f\"filtered_nr_shap_peaks_map_{short_feature}\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the nr. of peaks after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "\n",
    "    pd.Series(\n",
    "        dict(\n",
    "            zip(\n",
    "                *np.unique(\n",
    "                    [len(indices) for indices in plot_data[\"valid_peak_indices\"]],\n",
    "                    return_counts=True,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ).plot.bar(\n",
    "        ax=plt.subplots(figsize=(6, 4))[1],\n",
    "        title=f\"{short_feature}, {exc_name}\",\n",
    "        rot=0,\n",
    "    )\n",
    "    figure_saver.save_figure(\n",
    "        plt.gcf(), \"filtered_n_peaks_distr\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of specific peak combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 0.3 * len(relative_counts_dict) + 0.4))\n",
    "    pd.Series(\n",
    "        {\", \".join(k): v for k, v in relative_counts_dict.items()}\n",
    "    ).sort_values().plot.barh(\n",
    "        fontsize=12,\n",
    "        title=f\"{short_feature}, {exc_name}\",\n",
    "    )\n",
    "    plt.grid(alpha=0.4, linestyle=\"--\")\n",
    "    figure_saver.save_figure(plt.gcf(), \"peak_comb_distr\", sub_directory=sub_directory)\n",
    "    if close_figs:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of the most prominent peak combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "    peak_keys = plot_data[\"peak_keys\"]\n",
    "    imp_peaks = plot_data[\"imp_peaks\"]\n",
    "\n",
    "    boundaries = np.arange(len(peak_keys) + 1) - 0.5\n",
    "\n",
    "    cmap, norm = from_levels_and_colors(\n",
    "        levels=boundaries,\n",
    "        colors=[plt.get_cmap(\"tab10\")(i) for i in range(len(peak_keys))],\n",
    "        extend=\"neither\",\n",
    "    )\n",
    "\n",
    "    fig, cbar = cube_plotting(\n",
    "        imp_peaks,\n",
    "        title=f\"Peak Distr. {short_feature}, {exc_name}\",\n",
    "        fig=plt.figure(figsize=(5.1, 2.6)),\n",
    "        coastline_kwargs={\"linewidth\": 0.3},\n",
    "        colorbar_kwargs={\"label\": \"peak combination\"},\n",
    "        return_cbar=True,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "    )\n",
    "\n",
    "    tick_pos = np.arange(len(peak_keys), dtype=np.float64)\n",
    "    cbar.set_ticks(tick_pos)\n",
    "\n",
    "    cbar.set_ticklabels(peak_keys)\n",
    "\n",
    "    map_figure_saver.save_figure(\n",
    "        fig, f\"shap_peak_distr_map_{short_feature}\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined figure for the shap peak distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2, 2, figsize=(10.2, 5.2), subplot_kw=dict(projection=ccrs.Robinson())\n",
    ")\n",
    "\n",
    "all_labels = set()\n",
    "\n",
    "for ax, plot_data, title in zip(\n",
    "    axes.flatten(),\n",
    "    peak_data_iter(),\n",
    "    (f\"({l})\" for l in ascii_lowercase[: len(axes.flatten())]),\n",
    "):\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "    peak_keys = plot_data[\"peak_keys\"]\n",
    "    imp_peaks = plot_data[\"imp_peaks\"]\n",
    "\n",
    "    boundaries = np.arange(len(peak_keys) + 1) - 0.5\n",
    "\n",
    "    cmap, norm = from_levels_and_colors(\n",
    "        levels=boundaries,\n",
    "        colors=[plt.get_cmap(\"tab10\")(i) for i in range(len(peak_keys))],\n",
    "        extend=\"neither\",\n",
    "    )\n",
    "\n",
    "    _, cbar = cube_plotting(\n",
    "        imp_peaks,\n",
    "        title=\"\",\n",
    "        # title=f\"Peak Distr. {short_feature}, {exc_name}\",\n",
    "        coastline_kwargs={\"linewidth\": 0.3},\n",
    "        colorbar_kwargs={\n",
    "            # \"label\": \"peak combination\",\n",
    "            \"label\": \"\",\n",
    "            \"pad\": 0.02,\n",
    "        },\n",
    "        return_cbar=True,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    tick_pos = np.arange(len(peak_keys), dtype=np.float64)\n",
    "    cbar.set_ticks(tick_pos)\n",
    "\n",
    "    cbar_labels = [\", \".join(ps) for ps in peak_keys]\n",
    "    cbar.set_ticklabels(cbar_labels)\n",
    "    all_labels.update(cbar_labels)\n",
    "\n",
    "    ax.text(0.02, 0.925, title, transform=ax.transAxes)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.06, hspace=-0.25)\n",
    "\n",
    "# Save the combined figure.\n",
    "# map_figure_saver.save_figure(\n",
    "#     fig, \"normal_ba_peak_distr_max_shap_fapar_dry_days\", sub_directory=Path(\"shap_peaks\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group peaks into categories based on a DD (inst.) pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = list(islice(peak_data_iter(), 2, 3))[0]\n",
    "short_feature = plot_data[\"short_feature\"]\n",
    "exc_name = plot_data[\"exc_name\"]\n",
    "sub_directory = plot_data[\"sub_directory\"]\n",
    "relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "peak_keys = plot_data[\"peak_keys\"]\n",
    "raw_sel = imp_peaks = plot_data[\"imp_peaks\"].copy()\n",
    "\n",
    "sel = np.ma.MaskedArray(np.zeros_like(imp_peaks.data, dtype=np.int32), mask=True)\n",
    "\n",
    "# Dilate the 0s and 1s separately.\n",
    "for val in range(2):\n",
    "    dilated = binary_dilation(\n",
    "        (raw_sel == val).data.copy(), iterations=4, mask=imp_peaks.mask.copy()\n",
    "    )\n",
    "    sel[dilated.astype(\"bool\")] = val\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.pcolormesh(sel)\n",
    "_ = plt.title(f\"({short_feature}) Selection Overlap Template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the degree to which each peak combination overlaps with the above template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cat_dict():\n",
    "    return {\n",
    "        \"dd_0\": set(),\n",
    "        \"dd_1\": set(),\n",
    "        \"dd_mix\": set(),\n",
    "    }\n",
    "\n",
    "\n",
    "categories = defaultdict(gen_cat_dict)\n",
    "cat_thres = 0.7\n",
    "\n",
    "cat_masks = {}\n",
    "\n",
    "for plot_data in islice(peak_data_iter(), 0, None):\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "    peak_keys = plot_data[\"peak_keys\"]\n",
    "    imp_peaks = plot_data[\"imp_peaks\"]\n",
    "    peak_comb_df = plot_data[\"peak_comb_df\"]\n",
    "\n",
    "    overlap_data = {}\n",
    "    for i in np.unique(get_unmasked(imp_peaks)):\n",
    "        joined_key = \"|\".join(peak_keys[int(i)])\n",
    "\n",
    "        new_mask = imp_peaks == i\n",
    "        mask_key = (short_feature, joined_key)\n",
    "        if mask_key in cat_masks:\n",
    "            # Only overwrite an existing key if the new overlap region is larger.\n",
    "            if np.sum(new_mask) > np.sum(cat_masks[mask_key]):\n",
    "                cat_masks[mask_key] = new_mask\n",
    "        else:\n",
    "            # Assign new key.\n",
    "            cat_masks[mask_key] = new_mask\n",
    "\n",
    "        overlap_data[joined_key] = pd.Series(\n",
    "            get_unmasked(sel[cat_masks[(short_feature, joined_key)]])\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(overlap_data)\n",
    "    counts = df.apply(pd.Series.value_counts)\n",
    "    counts /= counts.sum(axis=0)\n",
    "\n",
    "    for name, data in counts.iteritems():\n",
    "        above_thres = data > cat_thres\n",
    "        if np.any(above_thres):\n",
    "            if np.isclose(data.index[np.where(above_thres)[0][0]], 0):\n",
    "                categories[short_feature][\"dd_0\"].add(name)\n",
    "            else:\n",
    "                categories[short_feature][\"dd_1\"].add(name)\n",
    "        else:\n",
    "            categories[short_feature][\"dd_mix\"].add(name)\n",
    "\n",
    "    counts.columns.name = short_feature\n",
    "    display_html(counts)\n",
    "\n",
    "cat_df = pd.DataFrame(categories)\n",
    "for _, peak_series in cat_df.iteritems():\n",
    "    for peaks_a, peaks_b in combinations(peak_series, 2):\n",
    "        # Each peak should only appear in a single category.\n",
    "        assert not peaks_a.intersection(peaks_b)\n",
    "cat_df = cat_df.applymap(tuple)\n",
    "cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_count_list_items(s):\n",
    "    count = 0\n",
    "    for item in s:\n",
    "        count += len(item)\n",
    "    return count\n",
    "\n",
    "\n",
    "cat_df.apply(total_count_list_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_list_items(s):\n",
    "    counts = []\n",
    "    for item in s:\n",
    "        counts.append(len(item))\n",
    "    return counts\n",
    "\n",
    "\n",
    "cat_df.apply(count_list_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cs_list = {\n",
    "    # H:[86, 266], C:[20,100], L:[40, 95]\n",
    "    \"4 cool\": [\"#7b5a19\", \"#a4c906\", \"#d9c398\", \"#008f91\"],\n",
    "    # H : 28, C : 123, L : 65\n",
    "    \"1 warm\": [\"#FB780F\"],\n",
    "    # H:[270, 10], C:[20, 100], L:[40, 95]\n",
    "    \"3 intermediate\": [\"#ca99a5\", \"#e32266\", \"#565a91\"],\n",
    "}\n",
    "\n",
    "\n",
    "def generate_cmap(cs):\n",
    "    if len(cs) == 1:\n",
    "        # Workaround to use the below method for a single color.\n",
    "        cs = cs * 2\n",
    "    return LinearSegmentedColormap.from_list(\"test\", cs, N=len(cs))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(len(cs_list), figsize=(8, 3))\n",
    "for (ax, (title, cs)) in zip(axes, cs_list.items()):\n",
    "    ax.imshow(gradient, aspect=\"auto\", cmap=generate_cmap(cs))\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "fig.tight_layout(h_pad=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {\n",
    "    \"dd_0\": \"1 warm\",\n",
    "    \"dd_1\": \"4 cool\",\n",
    "    \"dd_mix\": \"3 intermediate\",\n",
    "}\n",
    "\n",
    "color_s = pd.Series(category_map).apply(lambda x: cs_list[x])\n",
    "color_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_peaks(peaks, sep=\"|\"):\n",
    "    \"\"\"Sort peak combinations by first sign and month.\n",
    "\n",
    "    Examples:\n",
    "        >>> sort_peaks(['0(+)', '0(-)', '12(+)|0(+)', '12(+)|0(-)', '6(-)', '3(+)'])\n",
    "        ['0(+)', '3(+)', '12(+)|0(+)', '12(+)|0(-)', '0(-)', '6(-)']\n",
    "\n",
    "    \"\"\"\n",
    "    pos_indices = []\n",
    "    neg_indices = []\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        if \"+\" in peak.split(sep)[0]:\n",
    "            pos_indices.append(i)\n",
    "        else:\n",
    "            neg_indices.append(i)\n",
    "\n",
    "    pos_peaks = [peaks[i] for i in pos_indices]\n",
    "    neg_peaks = [peaks[i] for i in neg_indices]\n",
    "\n",
    "    def split_peak(peak):\n",
    "        out = []\n",
    "        for p in peak.split(sep):\n",
    "            out.extend([int(p[: p.find(\"(\")]), 0 if \"+\" in p else 1])\n",
    "        return tuple(out)\n",
    "\n",
    "    return sorted(pos_peaks, key=split_peak) + sorted(neg_peaks, key=split_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build mapping from peaks to colors.\n",
    "\n",
    "Each category (`dd_0`, etc...) has to be processed for both features simultaneously to coordinate the assignment of the same colours to the most similar (geographically) arrangement of peak patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_color_data = defaultdict(dict)\n",
    "\n",
    "for cat, peaks in cat_df.iterrows():\n",
    "    colors = deepcopy(color_s[cat])\n",
    "    peaks = deepcopy(peaks).apply(sort_peaks)\n",
    "\n",
    "    # Number of shared peaks = lowest number of peaks.\n",
    "    n_peaks = tuple(map(len, peaks))\n",
    "    shared_n = min(n_peaks)\n",
    "\n",
    "    if shared_n == 0 or all(n == 1 for n in n_peaks):\n",
    "        # There is nothing to do - simply assign the colors to the peaks.\n",
    "        for feature, peak_list in peaks.iteritems():\n",
    "            for peak, color in zip(peak_list, colors):\n",
    "                peak_color_data[feature][peak] = color\n",
    "        continue\n",
    "\n",
    "    # There are shared peaks (both features have > 0). Determine which\n",
    "    # of the colors to assign to which peaks to maximise the overlap.\n",
    "\n",
    "    # Compute the overlap matrix.\n",
    "    features = peaks.index\n",
    "    shape = tuple(map(len, (peaks[feature] for feature in features)))\n",
    "    overlaps = np.zeros(shape, dtype=np.int64)\n",
    "\n",
    "    peak_lists = deepcopy(dict(peaks.iteritems()))\n",
    "\n",
    "    for i, peak_i in enumerate(peaks[features[0]]):\n",
    "        mask_i = cat_masks[(features[0], peak_i)]\n",
    "        for j, peak_j in enumerate(peaks[features[1]]):\n",
    "            mask_j = cat_masks[(features[1], peak_j)]\n",
    "            overlaps[i, j] = np.sum(mask_i & mask_j)\n",
    "\n",
    "    # Iteratively use the largest (remaining) overlap to guide color assignment.\n",
    "\n",
    "    # We can only do this as many times as the shortest nr. of peaks.\n",
    "    for i in range(min(shape)):\n",
    "        # Determine the largest overlap.\n",
    "\n",
    "        # These two peaks will have the same color.\n",
    "        max_indices = np.unravel_index(np.argmax(overlaps), shape)\n",
    "        new_color = colors.pop()\n",
    "        for i, new_feature in enumerate(features):\n",
    "            new_peak = peaks[new_feature][max_indices[i]]\n",
    "            peak_lists[new_feature].remove(new_peak)\n",
    "            peak_color_data[new_feature][new_peak] = new_color\n",
    "\n",
    "            # Ensure these peaks are not used again by setting the corresponding row / column to -1.\n",
    "            full_slice = [slice(None)] * 2\n",
    "            full_slice[i] = max_indices[i]\n",
    "            overlaps[tuple(full_slice)] = -1\n",
    "\n",
    "    # Fill out any remaining colours.\n",
    "    for p_i in range(max(map(len, peak_lists.values()))):\n",
    "        new_color = colors.pop()\n",
    "        for i, new_feature in enumerate(features):\n",
    "            if peak_lists[new_feature]:\n",
    "                # Only add a new color if there are still peaks to add colors to.\n",
    "                new_peak = peak_lists[new_feature].pop()\n",
    "                peak_color_data[new_feature][new_peak] = new_color\n",
    "\n",
    "peak_color_data = dict(peak_color_data)\n",
    "peak_color_df = pd.DataFrame(peak_color_data)\n",
    "peak_color_df = peak_color_df.reindex(sort_peaks(peak_color_df.index))\n",
    "peak_color_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the combined colorbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "fig, axes = plt.subplots(peak_color_df.shape[1], figsize=(8, 1.8))\n",
    "for (ax, (feature, color_s)) in zip(axes, peak_color_df.iteritems()):\n",
    "    cs = color_s.dropna()\n",
    "\n",
    "    # Separate peak keys into the categories.\n",
    "    grouped_sorted_peak_keys = []\n",
    "    for category in [\"dd_0\", \"dd_1\", \"dd_mix\"]:\n",
    "        for peak_key in cs.index:\n",
    "            if peak_key in cat_df[feature][category]:\n",
    "                grouped_sorted_peak_keys.append(peak_key)\n",
    "\n",
    "    cs = cs.reindex(grouped_sorted_peak_keys)\n",
    "\n",
    "    ax.imshow(gradient, aspect=\"auto\", cmap=generate_cmap(cs))\n",
    "    ax.set_title(feature)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.tick_params(left=False, labelleft=False)\n",
    "    ax.set_xticks(get_centres(np.linspace(0, 256, len(cs) + 1)))\n",
    "    ax.set_xticklabels(cs.index)\n",
    "\n",
    "fig.tight_layout(h_pad=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    2,\n",
    "    2,\n",
    "    figsize=(9.4, 4.68),\n",
    "    subplot_kw=dict(projection=ccrs.Robinson()),\n",
    "    # XXX:\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "for ax, plot_data, title in zip(\n",
    "    axes.T.flatten(),\n",
    "    peak_data_iter(),\n",
    "    ascii_lowercase,\n",
    "):\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "    peak_keys = plot_data[\"peak_keys\"]\n",
    "    imp_peaks = plot_data[\"imp_peaks\"].astype(\"int\")\n",
    "\n",
    "    boundaries = np.arange(len(peak_keys) + 1) - 0.5\n",
    "\n",
    "    ungrouped_sorted_peak_keys = list(\n",
    "        map(\n",
    "            lambda p: tuple(p.split(\"|\")),\n",
    "            sort_peaks(tuple(map(lambda ps: \"|\".join(ps), peak_keys))),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Separate peak keys into the categories.\n",
    "    grouped_sorted_peak_keys = []\n",
    "    for category in [\"dd_0\", \"dd_1\", \"dd_mix\"]:\n",
    "        for peak_key in ungrouped_sorted_peak_keys:\n",
    "            if \"|\".join(peak_key) in cat_df[short_feature][category]:\n",
    "                grouped_sorted_peak_keys.append(peak_key)\n",
    "    # Reverse to put the 'first' item on top of the colorbar.\n",
    "    grouped_sorted_peak_keys = grouped_sorted_peak_keys[::-1]\n",
    "    assert len(grouped_sorted_peak_keys) == len(ungrouped_sorted_peak_keys)\n",
    "\n",
    "    # Map from the old peak indices to the new sorted indices.\n",
    "    old_to_new = {}\n",
    "    for old in np.unique(get_unmasked(imp_peaks)):\n",
    "        old_to_new[old] = grouped_sorted_peak_keys.index(peak_keys[old])\n",
    "\n",
    "    sorted_imp_peaks = imp_peaks.copy()\n",
    "    for old, new in old_to_new.items():\n",
    "        sorted_imp_peaks[imp_peaks == old] = new\n",
    "\n",
    "    cmap, norm = from_levels_and_colors(\n",
    "        levels=boundaries,\n",
    "        colors=[\n",
    "            peak_color_df[short_feature][\"|\".join(peaks)]\n",
    "            for peaks in grouped_sorted_peak_keys\n",
    "        ],\n",
    "        extend=\"neither\",\n",
    "    )\n",
    "\n",
    "    cube_plotting(\n",
    "        sorted_imp_peaks,\n",
    "        title=\"\",\n",
    "        # title=f\"Peak Distr. {short_feature}, {exc_name}\",\n",
    "        colorbar_kwargs=False,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    if exclude_inst:\n",
    "        exc_string = f\"(no current {short_feature})\"\n",
    "    else:\n",
    "        exc_string = f\"(with current {short_feature})\"\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        1.05,\n",
    "        f\"({title}) {short_feature} {exc_string}\",\n",
    "        transform=ax.transAxes,\n",
    "        ha=\"center\",\n",
    "        va=\"center_baseline\",\n",
    "    )\n",
    "\n",
    "fig.subplots_adjust(wspace=-0.12, hspace=0.12)\n",
    "\n",
    "# Add the shared colorbars.\n",
    "\n",
    "for (i, (feature, color_s)) in enumerate(peak_color_df.iteritems()):\n",
    "    cs = color_s.copy().dropna()\n",
    "\n",
    "    # Separate peak keys into the categories.\n",
    "    grouped_sorted_peak_keys = []\n",
    "    for category in [\"dd_0\", \"dd_1\", \"dd_mix\"]:\n",
    "        for peak_key in cs.index:\n",
    "            if peak_key in cat_df[feature][category]:\n",
    "                grouped_sorted_peak_keys.append(peak_key)\n",
    "\n",
    "    cs = cs.reindex(grouped_sorted_peak_keys)\n",
    "\n",
    "    cmap = mpl.colors.ListedColormap(cs)\n",
    "    bounds = np.arange(0, len(cs) + 1)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    box = axes[-1, i].get_position()\n",
    "\n",
    "    height = 0.02\n",
    "    y0 = box.ymin - height - 0.04\n",
    "\n",
    "    width = 0.04 * len(cs)\n",
    "\n",
    "    x0 = (box.xmin + box.xmax) / 2 - width / 2\n",
    "\n",
    "    cb = mpl.colorbar.ColorbarBase(\n",
    "        fig.add_axes([x0, y0, width, height]),\n",
    "        cmap=cmap,\n",
    "        boundaries=bounds,\n",
    "        extend=\"neither\",\n",
    "        ticks=get_centres(bounds),\n",
    "        spacing=\"uniform\",\n",
    "        orientation=\"horizontal\",\n",
    "        label=f\"peak combination ({feature})\",\n",
    "    )\n",
    "    cb.ax.set_xticklabels(cs.index, rotation=35)\n",
    "    cb.ax.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# Save the combined figure.\n",
    "map_figure_saver.save_figure(\n",
    "    fig,\n",
    "    \"normal_ba_peak_distr_max_shap_fapar_dry_days\",\n",
    "    sub_directory=Path(\"shap_peaks\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots over each level (peak_combination or pft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "    peak_keys = plot_data[\"peak_keys\"]\n",
    "    imp_peaks = plot_data[\"imp_peaks\"]\n",
    "    peak_comb_df = plot_data[\"peak_comb_df\"]\n",
    "\n",
    "    for level in [0, 1]:\n",
    "        peak_comb_df.groupby(axis=1, level=level).boxplot(\n",
    "            subplots=True,\n",
    "            layout=(len(peak_comb_df.columns.levels[level]), 1),\n",
    "            figsize=(10, 5 * len(peak_comb_df.columns.levels[level])),\n",
    "            rot=30,\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        figure_saver.save_figure(\n",
    "            plt.gcf(),\n",
    "            f\"boxplots_level_{peak_comb_df.columns.names[level]}\",\n",
    "            sub_directory=sub_directory,\n",
    "        )\n",
    "        if close_figs:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin plots over each level (peak_combinaton or pft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"]\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "    peak_keys = plot_data[\"peak_keys\"]\n",
    "    imp_peaks = plot_data[\"imp_peaks\"]\n",
    "    peak_comb_df = plot_data[\"peak_comb_df\"]\n",
    "\n",
    "    for level in [0, 1]:\n",
    "        for key in peak_comb_df.columns.levels[level]:\n",
    "            fig = plt.figure()\n",
    "            sns.violinplot(data=peak_comb_df.xs(key, level=level, axis=\"columns\"))\n",
    "            plt.title(key)\n",
    "            figure_saver.save_figure(\n",
    "                fig,\n",
    "                f\"violin_level_{peak_comb_df.columns.names[level]}_{key}\",\n",
    "                sub_directory=sub_directory,\n",
    "            )\n",
    "            if close_figs:\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot aggregate statistics per PFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"] / \"agg_plots\"\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "    peak_keys = plot_data[\"peak_keys\"]\n",
    "    imp_peaks = plot_data[\"imp_peaks\"]\n",
    "    peak_comb_df = plot_data[\"peak_comb_df\"]\n",
    "\n",
    "    for pft_key in peak_comb_df.columns.levels[1]:\n",
    "        pft_df = peak_comb_df.xs(pft_key, level=\"pft\", axis=\"columns\")\n",
    "\n",
    "        all_cols = set()\n",
    "        stripped_cols = []\n",
    "        for col in pft_df.columns:\n",
    "            # Extract the individual peaks from the column strings.\n",
    "            stripped_cols.append(\n",
    "                [s for s in [s.strip().strip(\"'\") for s in col[1:-1].split(\",\")] if s]\n",
    "            )\n",
    "            all_cols.update(stripped_cols[-1])\n",
    "        all_cols = sorted(tuple(all_cols))\n",
    "\n",
    "        agg_data = {}\n",
    "        for col in all_cols:\n",
    "            selected_df = pft_df.loc[:, [col in cols for cols in stripped_cols]]\n",
    "            all_values = []\n",
    "            for s_col in selected_df.columns:\n",
    "                all_values.extend(\n",
    "                    selected_df[s_col][selected_df[s_col].notnull()].values\n",
    "                )\n",
    "            agg_data[col] = pd.Series(all_values)\n",
    "        agg_df = pd.DataFrame(agg_data)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        sns.violinplot(data=agg_df)\n",
    "        plt.title(pft_key)\n",
    "\n",
    "        figure_saver.save_figure(\n",
    "            fig,\n",
    "            f\"violin_plots_{pft_key}\",\n",
    "            sub_directory=sub_directory,\n",
    "        )\n",
    "        if close_figs:\n",
    "            plt.close()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        agg_df.boxplot()\n",
    "        plt.title(pft_key)\n",
    "\n",
    "        figure_saver.save_figure(\n",
    "            fig,\n",
    "            f\"box_plots_{pft_key}\",\n",
    "            sub_directory=sub_directory,\n",
    "        )\n",
    "        if close_figs:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse aggregate statistics per PFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_data in peak_data_iter():\n",
    "    short_feature = plot_data[\"short_feature\"]\n",
    "    exc_name = plot_data[\"exc_name\"]\n",
    "    sub_directory = plot_data[\"sub_directory\"] / \"agg_plots\"\n",
    "    relative_counts_dict = plot_data[\"relative_counts_dict\"]\n",
    "    peak_keys = plot_data[\"peak_keys\"]\n",
    "    imp_peaks = plot_data[\"imp_peaks\"]\n",
    "    peak_comb_df = plot_data[\"peak_comb_df\"]\n",
    "\n",
    "    agg_dfs = {}\n",
    "\n",
    "    for pft_key in peak_comb_df.columns.levels[1]:\n",
    "        pft_df = peak_comb_df.xs(pft_key, level=\"pft\", axis=\"columns\")\n",
    "\n",
    "        all_cols = set()\n",
    "        stripped_cols = []\n",
    "        for col in pft_df.columns:\n",
    "            # Extract the individual peaks from the column strings.\n",
    "            stripped_cols.append(\n",
    "                [s for s in [s.strip().strip(\"'\") for s in col[1:-1].split(\",\")] if s]\n",
    "            )\n",
    "            all_cols.update(stripped_cols[-1])\n",
    "        all_cols = sorted(tuple(all_cols))\n",
    "\n",
    "        agg_data = {}\n",
    "        for col in all_cols:\n",
    "            selected_df = pft_df.loc[:, [col in cols for cols in stripped_cols]]\n",
    "            all_values = []\n",
    "            for s_col in selected_df.columns:\n",
    "                all_values.extend(\n",
    "                    selected_df[s_col][selected_df[s_col].notnull()].values\n",
    "                )\n",
    "            agg_data[col] = pd.Series(all_values)\n",
    "        agg_df = pd.DataFrame(agg_data)\n",
    "        agg_dfs[pft_key] = agg_df\n",
    "\n",
    "medians = {}\n",
    "for key, agg_df in agg_dfs.items():\n",
    "    medians[key] = agg_df.median()\n",
    "\n",
    "median_df = pd.DataFrame(medians).T\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.heatmap(median_df, annot=True)\n",
    "figure_saver.save_figure(\n",
    "    fig,\n",
    "    \"medians\",\n",
    "    sub_directory=Path(\"shap_peaks\") / filter_name / shap_measure,\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "sns.heatmap(median_df / median_df.max(axis=1).values.reshape(-1, 1), annot=True)\n",
    "figure_saver.save_figure(\n",
    "    fig,\n",
    "    \"norm_medians\",\n",
    "    sub_directory=Path(\"shap_peaks\") / filter_name / shap_measure,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires]",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
