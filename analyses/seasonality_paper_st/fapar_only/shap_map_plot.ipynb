{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from specific import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    endog_data,\n",
    "    exog_data,\n",
    "    master_mask,\n",
    "    filled_datasets,\n",
    "    masked_datasets,\n",
    "    land_mask,\n",
    ") = get_offset_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve previous results from the 'model' notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_split_cache.load()\n",
    "rf = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap_cache.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BA in the train and validation sets\n",
    "\n",
    "Valid elements are situated where master_mask is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(shap_values.flatten()), bins=5000)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate 2D masked array SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_shap_results = calculate_2d_masked_shap_values(X_train, master_mask, shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting maps of SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_value_maps(\n",
    "    X_train, map_shap_results, map_figure_saver, directory=Path(\"shap_maps\") / \"normal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High BA month mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ba = get_masked_array(endog_data.values, master_mask)\n",
    "mean_ba = np.ma.mean(target_ba, axis=0)\n",
    "max_ba = np.ma.max(target_ba, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"weighted_shap_maps\") / \"high_ba\"\n",
    "\n",
    "high_ba_mask = (\n",
    "    ~((target_ba > (2 * np.ma.mean(target_ba, axis=0))) & (np.ma.max(target_ba) > 1e-2))\n",
    ").data\n",
    "high_ba_mask |= target_ba.mask\n",
    "high_ba_mask |= np.sum(high_ba_mask, axis=0) < 9\n",
    "\n",
    "high_ba_sum = np.sum(high_ba_mask, axis=0)\n",
    "\n",
    "plot_sum = np.ma.MaskedArray(12 - high_ba_sum, mask=np.isclose(high_ba_sum, 12))\n",
    "\n",
    "boundaries = np.arange(np.min(plot_sum), np.max(plot_sum) + 2) - 0.5\n",
    "fig, cbar = cube_plotting(\n",
    "    plot_sum,\n",
    "    boundaries=boundaries,\n",
    "    return_cbar=True,\n",
    "    colorbar_kwargs={\"label\": \"nr. valid samples\"},\n",
    "    fig=plt.figure(figsize=(5.1, 2.6)),\n",
    ")\n",
    "cbar.set_ticks(get_centres(boundaries))\n",
    "cbar.set_ticklabels(list(map(int, get_centres(boundaries))))\n",
    "\n",
    "map_figure_saver.save_figure(fig, \"high_ba_n_valid\", sub_directory=root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate SHAP results only for those voxels with high BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_ba_map_shap_results = calculate_2d_masked_shap_values(\n",
    "    X_train, master_mask, shap_values, additional_mask=high_ba_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting corresponding maps of high BA SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_value_maps(\n",
    "    X_train,\n",
    "    high_ba_map_shap_results,\n",
    "    map_figure_saver,\n",
    "    directory=Path(\"shap_maps\") / \"high_ba\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank SHAP masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_month = 9\n",
    "\n",
    "# Mark areas where at least this fraction of |SHAP| has been plotted previously.\n",
    "thres = 0.7\n",
    "\n",
    "close_figs = True\n",
    "\n",
    "\n",
    "def param_iter():\n",
    "    for (exc_name, exclude_inst) in tqdm(\n",
    "        [(\"with_inst\", False), (\"no_inst\", True)], desc=\"Exclude inst.\"\n",
    "    ):\n",
    "        for feature_name in tqdm(\n",
    "            [\"VOD Ku-band\", \"SIF\", \"FAPAR\", \"LAI\", \"Dry Day Period\"], desc=\"Feature\"\n",
    "        ):\n",
    "            for (filter_name, shap_results) in tqdm(\n",
    "                [(\"normal\", map_shap_results), (\"high_ba\", high_ba_map_shap_results)],\n",
    "                desc=\"SHAP results\",\n",
    "            ):\n",
    "                for shap_measure in tqdm(\n",
    "                    [\"masked_max_shap_arrs\", \"masked_abs_shap_arrs\"],\n",
    "                    desc=\"SHAP measure\",\n",
    "                ):\n",
    "                    yield (exc_name, exclude_inst), feature_name, (\n",
    "                        filter_name,\n",
    "                        shap_results,\n",
    "                    ), shap_measure\n",
    "\n",
    "\n",
    "for (\n",
    "    (exc_name, exclude_inst),\n",
    "    feature_name,\n",
    "    (filter_name, shap_results),\n",
    "    shap_measure,\n",
    ") in islice(param_iter(), 0, None):\n",
    "    short_feature = shorten_features(feature_name)\n",
    "\n",
    "    sub_directory = Path(\"rank_shap_maps\") / filter_name / shap_measure / exc_name\n",
    "\n",
    "    filtered = np.array(filter_by_month(X_train.columns, feature_name, max_month))\n",
    "    lags = np.array(\n",
    "        [get_lag(feature, target_feature=feature_name) for feature in filtered]\n",
    "    )\n",
    "\n",
    "    # Ensure lags are sorted consistently.\n",
    "    lag_sort_inds = np.argsort(lags)\n",
    "    filtered = tuple(filtered[lag_sort_inds])\n",
    "    lags = tuple(lags[lag_sort_inds])\n",
    "\n",
    "    if exclude_inst and 0 in lags:\n",
    "        assert lags[0] == 0\n",
    "        lags = lags[1:]\n",
    "        filtered = filtered[1:]\n",
    "\n",
    "    n_features = len(filtered)\n",
    "\n",
    "    # There is no point plotting this map for a single feature or less since we are\n",
    "    # interested in a comparison between different feature ranks.\n",
    "    if n_features <= 1:\n",
    "        continue\n",
    "\n",
    "    selected_data = np.empty(n_features, dtype=object)\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        if col in filtered:\n",
    "            selected_data[lags.index(get_lag(col))] = shap_results[shap_measure][\n",
    "                \"data\"\n",
    "            ][i].copy()\n",
    "\n",
    "    shared_mask = reduce(np.logical_or, (data.mask for data in selected_data))\n",
    "    for data in selected_data:\n",
    "        data.mask = shared_mask\n",
    "\n",
    "    stacked_abs = np.abs(np.vstack([data.data[np.newaxis] for data in selected_data]))\n",
    "    # Indices in descending order.\n",
    "    sort_indices = np.argsort(stacked_abs, axis=0)[::-1]\n",
    "\n",
    "    # Maintain the same colors even if fewer colors are used.\n",
    "    colors = [lag_color_dict[lag] for lag in lags]\n",
    "\n",
    "    cmap, norm = from_levels_and_colors(\n",
    "        levels=np.arange(n_features + 1), colors=colors, extend=\"neither\",\n",
    "    )\n",
    "\n",
    "    short_feature = shorten_features(feature_name)\n",
    "\n",
    "    sum_shap = np.ma.MaskedArray(np.sum(stacked_abs, axis=0), mask=shared_mask)\n",
    "    already_plotted = np.zeros_like(sum_shap)\n",
    "\n",
    "    for i, rank in zip(\n",
    "        tqdm(range(n_features), desc=\"Plotting\", leave=False),\n",
    "        [\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\"],\n",
    "    ):\n",
    "        cube = dummy_lat_lon_cube(np.ma.MaskedArray(sort_indices[i], mask=shared_mask))\n",
    "\n",
    "        fig, ax = plt.subplots(\n",
    "            figsize=(5.1, 2.6), subplot_kw={\"projection\": ccrs.Robinson()}\n",
    "        )\n",
    "\n",
    "        style = 2\n",
    "\n",
    "        if style == 1:\n",
    "            # Stippling for significant areas.\n",
    "            mpl.rc(\"hatch\", linewidth=0.2)\n",
    "            hatches = [\".\" * 6, None]\n",
    "        else:\n",
    "            # Hatching for insignificant areas.\n",
    "            mpl.rc(\"hatch\", linewidth=0.1)\n",
    "            hatches = [\"/\" * 14, None]\n",
    "\n",
    "        # XXX: Temporary, since this takes an exorbitant amount of time to render.\n",
    "        #         if np.any(already_plotted >= thres):\n",
    "        #             ax.contourf(\n",
    "        #                 cube.coord(\"longitude\").points,\n",
    "        #                 cube.coord(\"latitude\").points,\n",
    "        #                 already_plotted,\n",
    "        #                 transform=ccrs.PlateCarree(),\n",
    "        #                 colors=\"none\",\n",
    "        #                 zorder=4,\n",
    "        #                 levels=[thres, 1],\n",
    "        #                 hatches=hatches,\n",
    "        #             )\n",
    "\n",
    "        fig, cbar = cube_plotting(\n",
    "            cube,\n",
    "            title=f\"{rank} |SHAP {short_feature} Lag| - thres: {thres * 100:0.0f}%\",\n",
    "            fig=fig,\n",
    "            ax=ax,\n",
    "            cmap=cmap,\n",
    "            norm=norm,\n",
    "            return_cbar=True,\n",
    "            colorbar_kwargs={\"label\": short_feature},\n",
    "            coastline_kwargs={\"linewidth\": 0.3},\n",
    "        )\n",
    "\n",
    "        # Label the colorbar using the feature names.\n",
    "        cbar.set_ticks(np.arange(n_features) + 0.5)\n",
    "\n",
    "        labels = []\n",
    "        for lag in lags:\n",
    "            if lag:\n",
    "                labels.append(f\"{lag} M\")\n",
    "            else:\n",
    "                labels.append(\"Inst.\")\n",
    "        cbar.set_ticklabels(labels)\n",
    "\n",
    "        data = np.take_along_axis(stacked_abs, sort_indices[i : i + 1], axis=0)[0]\n",
    "        already_plotted += data / sum_shap\n",
    "\n",
    "        map_figure_saver.save_figure(\n",
    "            fig, f\"{rank}_{short_feature}\", sub_directory=sub_directory\n",
    "        )\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use significant peak finding algorithm to determine mean timing of maximum impact using SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate the role of the significance parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_month = 9\n",
    "\n",
    "# Mark areas where at least this fraction of |SHAP| has been plotted previously.\n",
    "thres = 0.7\n",
    "\n",
    "\n",
    "def param_iter():\n",
    "    for exclude_inst in tqdm([False, True], desc=\"Exclude inst.\"):\n",
    "        for feature_name in tqdm(\n",
    "            [\"VOD Ku-band\", \"SIF\", \"FAPAR\", \"LAI\", \"Dry Day Period\"], desc=\"Feature\"\n",
    "        ):\n",
    "            yield exclude_inst, feature_name\n",
    "\n",
    "\n",
    "for exclude_inst, feature_name in islice(param_iter(), 0, None):\n",
    "    filtered = np.array(filter_by_month(X_train.columns, feature_name, max_month))\n",
    "    lags = np.array(\n",
    "        [get_lag(feature, target_feature=feature_name) for feature in filtered]\n",
    "    )\n",
    "\n",
    "    # Ensure lags are sorted consistently.\n",
    "    lag_sort_inds = np.argsort(lags)\n",
    "    filtered = tuple(filtered[lag_sort_inds])\n",
    "    lags = tuple(lags[lag_sort_inds])\n",
    "\n",
    "    if exclude_inst and 0 in lags:\n",
    "        assert lags[0] == 0\n",
    "        lags = lags[1:]\n",
    "        filtered = filtered[1:]\n",
    "\n",
    "    n_features = len(filtered)\n",
    "\n",
    "    # There is no point plotting this map for a single feature or less since we are\n",
    "    # interested in a comparison between different feature ranks.\n",
    "    if n_features <= 1:\n",
    "        continue\n",
    "\n",
    "    short_feature = shorten_features(feature_name)\n",
    "\n",
    "    selected_data = np.empty(n_features, dtype=object)\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        if col in filtered:\n",
    "            selected_data[lags.index(get_lag(col))] = map_shap_results[\n",
    "                \"masked_abs_shap_arrs\"\n",
    "            ][\"data\"][i].copy()\n",
    "\n",
    "    shared_mask = reduce(np.logical_or, (data.mask for data in selected_data))\n",
    "    for data in selected_data:\n",
    "        data.mask = shared_mask\n",
    "\n",
    "    stacked_shaps = np.vstack([data.data[np.newaxis] for data in selected_data])\n",
    "\n",
    "    # Calculate the significance of the global maxima for each of the valid pixels.\n",
    "\n",
    "    # Valid indices are recorded in 'shared_mask'.\n",
    "\n",
    "    valid_i, valid_j = np.where(~shared_mask)\n",
    "    total_valid = len(valid_i)\n",
    "\n",
    "    def get_true_significances(kwargs):\n",
    "        ptp_threshold_factor = kwargs.pop(\"ptp_threshold_factor\")\n",
    "\n",
    "        significant = []\n",
    "        for i, j in zip(valid_i, valid_j):\n",
    "            ptp_threshold = ptp_threshold_factor * mean_ba[i, j]\n",
    "            significant.append(\n",
    "                significant_peak(\n",
    "                    stacked_shaps[:, i, j], ptp_threshold=ptp_threshold, **kwargs\n",
    "                )\n",
    "            )\n",
    "        return dict(zip(*np.unique(significant, return_counts=True)))[True]\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=get_ncpus()) as executor:\n",
    "        diff_thresholds = np.linspace(0.01, 0.99, 15)\n",
    "        ptp_threshold_factors = np.linspace(0, 1.5, 8)\n",
    "\n",
    "        kwargs_list = [\n",
    "            {\n",
    "                \"diff_threshold\": diff_threshold,\n",
    "                \"ptp_threshold_factor\": ptp_threshold_factor,\n",
    "            }\n",
    "            for diff_threshold, ptp_threshold_factor in product(\n",
    "                diff_thresholds, ptp_threshold_factors\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        fs = [executor.submit(get_true_significances, kwargs) for kwargs in kwargs_list]\n",
    "        for f in tqdm(\n",
    "            concurrent.futures.as_completed(fs),\n",
    "            total=len(fs),\n",
    "            desc=\"Varying significances\",\n",
    "            smoothing=0,\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        perc_sigs = 100 * np.array([f.result() for f in fs]) / total_valid\n",
    "\n",
    "        for ptp_threshold_factor in ptp_threshold_factors:\n",
    "            ptp_indices = [\n",
    "                i\n",
    "                for i, kwargs in enumerate(kwargs_list)\n",
    "                if np.isclose(kwargs[\"ptp_threshold_factor\"], ptp_threshold_factor)\n",
    "            ]\n",
    "            plt.plot(\n",
    "                diff_thresholds,\n",
    "                perc_sigs[ptp_indices],\n",
    "                label=f\"ptp_thres: {ptp_threshold_factor:0.2f}\",\n",
    "            )\n",
    "\n",
    "        plt.title(f\"{short_feature} - Exclude Inst: {exclude_inst}\")\n",
    "        plt.xlabel(\"Diff threshold\")\n",
    "        plt.ylabel(\"% significant\")\n",
    "        plt.grid(linestyle=\"--\", alpha=0.4)\n",
    "        plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot sig. maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_threshold = 0.5\n",
    "ptp_threshold_factor = 0.12  # relative the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot weighted SHAP peak locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_month = 9\n",
    "close_figs = True\n",
    "\n",
    "\n",
    "def param_iter():\n",
    "    for (exc_name, exclude_inst) in tqdm(\n",
    "        [(\"with_inst\", False), (\"no_inst\", True)], desc=\"Exclude inst.\"\n",
    "    ):\n",
    "        for feature_name in tqdm(\n",
    "            [\"VOD Ku-band\", \"SIF\", \"FAPAR\", \"LAI\", \"Dry Day Period\"], desc=\"Feature\"\n",
    "        ):\n",
    "            for (filter_name, shap_results) in tqdm(\n",
    "                [(\"normal\", map_shap_results), (\"high_ba\", high_ba_map_shap_results)],\n",
    "                desc=\"SHAP results\",\n",
    "            ):\n",
    "                for shap_measure in tqdm(\n",
    "                    [\n",
    "                        \"masked_max_shap_arrs\",\n",
    "                        \"masked_abs_shap_arrs\",\n",
    "                        \"masked_shap_arrs\",\n",
    "                    ],\n",
    "                    desc=\"SHAP measure\",\n",
    "                ):\n",
    "                    yield (exc_name, exclude_inst), feature_name, (\n",
    "                        filter_name,\n",
    "                        shap_results,\n",
    "                    ), shap_measure\n",
    "\n",
    "\n",
    "for (\n",
    "    (exc_name, exclude_inst),\n",
    "    feature_name,\n",
    "    (filter_name, shap_results),\n",
    "    shap_measure,\n",
    ") in islice(param_iter(), 0, None):\n",
    "    short_feature = shorten_features(feature_name)\n",
    "\n",
    "    sub_directory = Path(\"weighted_shap_maps\") / filter_name / shap_measure / exc_name\n",
    "\n",
    "    filtered = np.array(filter_by_month(X_train.columns, feature_name, max_month))\n",
    "    lags = np.array(\n",
    "        [get_lag(feature, target_feature=feature_name) for feature in filtered]\n",
    "    )\n",
    "\n",
    "    # Ensure lags are sorted consistently.\n",
    "    lag_sort_inds = np.argsort(lags)\n",
    "    filtered = tuple(filtered[lag_sort_inds])\n",
    "    lags = tuple(lags[lag_sort_inds])\n",
    "\n",
    "    if exclude_inst and 0 in lags:\n",
    "        assert lags[0] == 0\n",
    "        lags = lags[1:]\n",
    "        filtered = filtered[1:]\n",
    "\n",
    "    n_features = len(filtered)\n",
    "\n",
    "    # There is no point plotting this map for a single feature or less since we are\n",
    "    # interested in a comparison between different feature ranks.\n",
    "    if n_features <= 1:\n",
    "        continue\n",
    "\n",
    "    selected_data = np.empty(n_features, dtype=object)\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        if col in filtered:\n",
    "            selected_data[lags.index(get_lag(col))] = shap_results[shap_measure][\n",
    "                \"data\"\n",
    "            ][i].copy()\n",
    "\n",
    "    shared_mask = reduce(np.logical_or, (data.mask for data in selected_data))\n",
    "    for data in selected_data:\n",
    "        data.mask = shared_mask\n",
    "\n",
    "    stacked_shaps = np.vstack([data.data[np.newaxis] for data in selected_data])\n",
    "\n",
    "    # Calculate the significance of the global maxima for each of the valid pixels.\n",
    "\n",
    "    # Valid indices are recorded in 'shared_mask'.\n",
    "\n",
    "    valid_i, valid_j = np.where(~shared_mask)\n",
    "    total_valid = len(valid_i)\n",
    "\n",
    "    max_positions = np.ma.MaskedArray(\n",
    "        np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "    )\n",
    "    for i, j in zip(tqdm(valid_i, desc=\"Evaluating maxima\", smoothing=0), valid_j):\n",
    "        ptp_threshold = ptp_threshold_factor * mean_ba[i, j]\n",
    "        if significant_peak(\n",
    "            stacked_shaps[:, i, j],\n",
    "            diff_threshold=diff_threshold,\n",
    "            ptp_threshold=ptp_threshold,\n",
    "        ):\n",
    "            # If the maximum is significant, go on the calculate the weighted avg. of the signal.\n",
    "            max_positions[i, j] = np.sum(\n",
    "                np.array(lags) * np.abs(stacked_shaps[:, i, j])\n",
    "            ) / np.sum(np.abs(stacked_shaps[:, i, j]))\n",
    "\n",
    "    #\n",
    "    fig = cube_plotting(\n",
    "        max_positions,\n",
    "        title=f\"|SHAP| Weighted Maximum {short_feature} - Exclude Inst: {exclude_inst}\",\n",
    "        fig=plt.figure(figsize=(5.1, 2.6)),\n",
    "        colorbar_kwargs={\"label\": short_feature, \"format\": \"%0.1f\"},\n",
    "        coastline_kwargs={\"linewidth\": 0.3},\n",
    "        #         boundaries=np.arange(0, max(lags)+1),\n",
    "    )\n",
    "    map_figure_saver.save_figure(\n",
    "        fig, f\"weighted_shap_{short_feature}\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()\n",
    "    #\n",
    "\n",
    "    # Plot example series.\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.title(f\"{short_feature} - Exclude Inst: {exclude_inst}\")\n",
    "#     for i in np.random.RandomState(0).choice(total_valid, 100, False):\n",
    "#         plot_i = valid_i[i]\n",
    "#         plot_j = valid_j[i]\n",
    "#         plot_data = stacked_shaps[:, plot_i, plot_j]\n",
    "#         plt.plot(lags, plot_data / np.max(plot_data), c=\"C0\", alpha=0.4)\n",
    "#     plt.xlabel(lags)\n",
    "#     plt.ylabel(\"|SHAP|\")\n",
    "#     plt.grid(linestyle=\"--\", alpha=0.4)\n",
    "# #     plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse SHAP -- BA relationship for selected regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_cmap = plt.get_cmap(\"inferno\")\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    1, 1, figsize=(14, 6), subplot_kw=dict(projection=ccrs.Robinson())\n",
    ")\n",
    "\n",
    "feature_name = \"Dry Days\"\n",
    "results_dict = map_shap_results[\"masked_shap_arrs\"]\n",
    "fig = cube_plotting(\n",
    "    results_dict[\"data\"][\n",
    "        list(map(shorten_features, X_train.columns)).index(feature_name)\n",
    "    ],\n",
    "    fig=fig,\n",
    "    ax=ax,\n",
    "    title=f\"Mean SHAP value for '{shorten_features(feature_name)}'\",\n",
    "    nbins=7,\n",
    "    vmin=results_dict[\"vmin\"],\n",
    "    vmax=results_dict[\"vmax\"],\n",
    "    log=True,\n",
    "    log_auto_bins=False,\n",
    "    extend=\"neither\",\n",
    "    min_edge=1e-3,\n",
    "    cmap=\"Spectral_r\",\n",
    "    cmap_midpoint=0,\n",
    "    cmap_symmetric=True,\n",
    "    colorbar_kwargs={\n",
    "        \"format\": \"%0.1e\",\n",
    "        \"label\": f\"SHAP ('{shorten_features(feature_name)}')\",\n",
    "    },\n",
    "    coastline_kwargs={\"linewidth\": 0.3},\n",
    ")\n",
    "\n",
    "# lat_range = (-5, 4)\n",
    "lat_range = (-5, 0)\n",
    "lon_range = (12.5, 26.5)\n",
    "\n",
    "ax.set_global()\n",
    "ax.add_patch(\n",
    "    mpatches.Rectangle(\n",
    "        xy=[min(lon_range), min(lat_range)],\n",
    "        width=np.ptp(lon_range),\n",
    "        height=np.ptp(lat_range),\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"blue\",\n",
    "        alpha=0.8,\n",
    "        lw=2,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# _ = ax.gridlines()\n",
    "\n",
    "ba_train_cube = dummy_lat_lon_cube(get_mm_data(y_train.values, master_mask, \"train\"))\n",
    "ba_subset = ba_train_cube.intersection(latitude=lat_range).intersection(\n",
    "    longitude=lon_range\n",
    ")\n",
    "cube_plotting(ba_subset, log=True)\n",
    "\n",
    "mm_valid_indices, mm_valid_train_indices, mm_valid_val_indices = get_mm_indices(\n",
    "    master_mask\n",
    ")\n",
    "mm_kind_indices = mm_valid_train_indices[: shap_values.shape[0]]\n",
    "X_i = list(map(shorten_features, X_train.columns)).index(feature_name)\n",
    "\n",
    "# Convert 1D shap values into 3D array (time, lat, lon).\n",
    "masked_shap_comp = np.ma.MaskedArray(\n",
    "    np.zeros_like(master_mask, dtype=np.float64), mask=np.ones_like(master_mask)\n",
    ")\n",
    "masked_shap_comp.ravel()[mm_kind_indices] = shap_values[:, X_i]\n",
    "\n",
    "# if additional_mask is not None:\n",
    "#     masked_shap_comp.mask |= match_shape(additional_mask, masked_shap_comp.shape)\n",
    "\n",
    "assert np.all(\n",
    "    np.isclose(results_dict[\"data\"][X_i], np.ma.mean(masked_shap_comp, axis=0))\n",
    ")\n",
    "\n",
    "shap_cube = dummy_lat_lon_cube(masked_shap_comp)\n",
    "shap_subset = shap_cube.intersection(latitude=lat_range).intersection(\n",
    "    longitude=lon_range\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "indices = list(np.ndindex(ba_subset.shape[1:]))\n",
    "ba_camap = plt.get_cmap(\"inferno\")\n",
    "for i, index in enumerate(tqdm(indices, desc=\"Plotting\")):\n",
    "    plt.plot(\n",
    "        ba_subset.data[(slice(None), *index)],\n",
    "        marker=\"o\",\n",
    "        alpha=0.4,\n",
    "        c=ba_cmap(i / (len(indices) - 1)),\n",
    "    )\n",
    "_ = plt.ylabel(\"BA\")\n",
    "\n",
    "plt.figure()\n",
    "indices = list(np.ndindex(ba_subset.shape[1:]))\n",
    "ba_camap = plt.get_cmap(\"inferno\")\n",
    "for i, index in enumerate(tqdm(indices, desc=\"Plotting\")):\n",
    "    plt.plot(\n",
    "        shap_subset.data[(slice(None), *index)],\n",
    "        marker=\"o\",\n",
    "        alpha=0.4,\n",
    "        c=ba_cmap(i / (len(indices) - 1)),\n",
    "    )\n",
    "_ = plt.ylabel(\"SHAP(Dry Days)\")\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "indices = list(np.ndindex(ba_subset.shape[1:]))\n",
    "ba_camap = plt.get_cmap(\"inferno\")\n",
    "for i, index in enumerate(tqdm(indices, desc=\"Plotting\")):\n",
    "    plt.plot(\n",
    "        shap_subset.data[(slice(None), *index)],\n",
    "        ba_subset.data[(slice(None), *index)],\n",
    "        marker=\"o\",\n",
    "        alpha=0.4,\n",
    "        c=ba_cmap(i / (len(indices) - 1)),\n",
    "        linestyle=\"\",\n",
    "    )\n",
    "plt.ylabel(\"BA\")\n",
    "_ = plt.xlabel(\"SHAP(Dry Days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    1, 1, figsize=(14, 6), subplot_kw=dict(projection=ccrs.Robinson())\n",
    ")\n",
    "\n",
    "feature_name = \"Dry Days\"\n",
    "results_dict = map_shap_results[\"masked_shap_arrs\"]\n",
    "fig = cube_plotting(\n",
    "    results_dict[\"data\"][\n",
    "        list(map(shorten_features, X_train.columns)).index(feature_name)\n",
    "    ],\n",
    "    fig=fig,\n",
    "    ax=ax,\n",
    "    title=f\"Mean SHAP value for '{shorten_features(feature_name)}'\",\n",
    "    nbins=7,\n",
    "    vmin=results_dict[\"vmin\"],\n",
    "    vmax=results_dict[\"vmax\"],\n",
    "    log=True,\n",
    "    log_auto_bins=False,\n",
    "    extend=\"neither\",\n",
    "    min_edge=1e-3,\n",
    "    cmap=\"Spectral_r\",\n",
    "    cmap_midpoint=0,\n",
    "    cmap_symmetric=True,\n",
    "    colorbar_kwargs={\n",
    "        \"format\": \"%0.1e\",\n",
    "        \"label\": f\"SHAP ('{shorten_features(feature_name)}')\",\n",
    "    },\n",
    "    coastline_kwargs={\"linewidth\": 0.3},\n",
    ")\n",
    "\n",
    "# lat_range = (-5, 4)\n",
    "lat_range = (6, 11)\n",
    "lon_range = (12.5, 26.5)\n",
    "\n",
    "ax.set_global()\n",
    "ax.add_patch(\n",
    "    mpatches.Rectangle(\n",
    "        xy=[min(lon_range), min(lat_range)],\n",
    "        width=np.ptp(lon_range),\n",
    "        height=np.ptp(lat_range),\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"blue\",\n",
    "        alpha=0.8,\n",
    "        lw=2,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# _ = ax.gridlines()\n",
    "\n",
    "ba_train_cube = dummy_lat_lon_cube(get_mm_data(y_train.values, master_mask, \"train\"))\n",
    "ba_subset = ba_train_cube.intersection(latitude=lat_range).intersection(\n",
    "    longitude=lon_range\n",
    ")\n",
    "cube_plotting(ba_subset, log=True)\n",
    "\n",
    "mm_valid_indices, mm_valid_train_indices, mm_valid_val_indices = get_mm_indices(\n",
    "    master_mask\n",
    ")\n",
    "mm_kind_indices = mm_valid_train_indices[: shap_values.shape[0]]\n",
    "X_i = list(map(shorten_features, X_train.columns)).index(feature_name)\n",
    "\n",
    "# Convert 1D shap values into 3D array (time, lat, lon).\n",
    "masked_shap_comp = np.ma.MaskedArray(\n",
    "    np.zeros_like(master_mask, dtype=np.float64), mask=np.ones_like(master_mask)\n",
    ")\n",
    "masked_shap_comp.ravel()[mm_kind_indices] = shap_values[:, X_i]\n",
    "\n",
    "# if additional_mask is not None:\n",
    "#     masked_shap_comp.mask |= match_shape(additional_mask, masked_shap_comp.shape)\n",
    "\n",
    "assert np.all(\n",
    "    np.isclose(results_dict[\"data\"][X_i], np.ma.mean(masked_shap_comp, axis=0))\n",
    ")\n",
    "\n",
    "shap_cube = dummy_lat_lon_cube(masked_shap_comp)\n",
    "shap_subset = shap_cube.intersection(latitude=lat_range).intersection(\n",
    "    longitude=lon_range\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "indices = list(np.ndindex(ba_subset.shape[1:]))\n",
    "ba_camap = plt.get_cmap(\"inferno\")\n",
    "for i, index in enumerate(tqdm(indices, desc=\"Plotting\")):\n",
    "    plt.plot(\n",
    "        ba_subset.data[(slice(None), *index)],\n",
    "        marker=\"o\",\n",
    "        alpha=0.4,\n",
    "        c=ba_cmap(i / (len(indices) - 1)),\n",
    "    )\n",
    "_ = plt.ylabel(\"BA\")\n",
    "\n",
    "plt.figure()\n",
    "indices = list(np.ndindex(ba_subset.shape[1:]))\n",
    "ba_camap = plt.get_cmap(\"inferno\")\n",
    "for i, index in enumerate(tqdm(indices, desc=\"Plotting\")):\n",
    "    plt.plot(\n",
    "        shap_subset.data[(slice(None), *index)],\n",
    "        marker=\"o\",\n",
    "        alpha=0.4,\n",
    "        c=ba_cmap(i / (len(indices) - 1)),\n",
    "    )\n",
    "_ = plt.ylabel(\"SHAP(Dry Days)\")\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "indices = list(np.ndindex(ba_subset.shape[1:]))\n",
    "ba_camap = plt.get_cmap(\"inferno\")\n",
    "for i, index in enumerate(tqdm(indices, desc=\"Plotting\")):\n",
    "    plt.plot(\n",
    "        shap_subset.data[(slice(None), *index)],\n",
    "        ba_subset.data[(slice(None), *index)],\n",
    "        marker=\"o\",\n",
    "        alpha=0.4,\n",
    "        c=ba_cmap(i / (len(indices) - 1)),\n",
    "        linestyle=\"\",\n",
    "    )\n",
    "plt.ylabel(\"BA\")\n",
    "_ = plt.xlabel(\"SHAP(Dry Days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorisation into multiple peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the effect of parameters on the peak distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_month = 9\n",
    "\n",
    "# Mark areas where at least this fraction of |SHAP| has been plotted previously.\n",
    "thres = 0.7\n",
    "\n",
    "\n",
    "def param_iter():\n",
    "    for exclude_inst in tqdm([False], desc=\"Exclude inst.\"):\n",
    "        for feature_name in tqdm([\"FAPAR\", \"Dry Day Period\"], desc=\"Feature\"):\n",
    "            yield exclude_inst, feature_name\n",
    "\n",
    "\n",
    "for exclude_inst, feature_name in islice(param_iter(), 0, 1):\n",
    "    filtered = np.array(filter_by_month(X_train.columns, feature_name, max_month))\n",
    "    lags = np.array(\n",
    "        [get_lag(feature, target_feature=feature_name) for feature in filtered]\n",
    "    )\n",
    "\n",
    "    # Ensure lags are sorted consistently.\n",
    "    lag_sort_inds = np.argsort(lags)\n",
    "    filtered = tuple(filtered[lag_sort_inds])\n",
    "    lags = tuple(lags[lag_sort_inds])\n",
    "\n",
    "    if exclude_inst and 0 in lags:\n",
    "        assert lags[0] == 0\n",
    "        lags = lags[1:]\n",
    "        filtered = filtered[1:]\n",
    "\n",
    "    n_features = len(filtered)\n",
    "\n",
    "    # There is no point plotting this map for a single feature or less since we are\n",
    "    # interested in a comparison between different feature ranks.\n",
    "    if n_features <= 1:\n",
    "        continue\n",
    "\n",
    "    short_feature = shorten_features(feature_name)\n",
    "\n",
    "    selected_data = np.empty(n_features, dtype=object)\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        if col in filtered:\n",
    "            selected_data[lags.index(get_lag(col))] = map_shap_results[\n",
    "                \"masked_shap_arrs\"\n",
    "            ][\"data\"][i].copy()\n",
    "\n",
    "    shared_mask = reduce(np.logical_or, (data.mask for data in selected_data))\n",
    "    for data in selected_data:\n",
    "        data.mask = shared_mask\n",
    "\n",
    "    stacked_shaps = np.vstack([data.data[np.newaxis] for data in selected_data])\n",
    "\n",
    "    # Calculate the significance of the global maxima for each of the valid pixels.\n",
    "\n",
    "    # Valid indices are recorded in 'shared_mask'.\n",
    "\n",
    "    valid_i, valid_j = np.where(~shared_mask)\n",
    "    total_valid = len(valid_i)\n",
    "\n",
    "    max_positions = np.ma.MaskedArray(\n",
    "        np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "    )\n",
    "\n",
    "    def get_n_peaks(ptp_threshold_factor, diff_threshold):\n",
    "        n_peaks = []\n",
    "\n",
    "        for i, j in zip(valid_i, valid_j):\n",
    "            ptp_threshold = ptp_threshold_factor * mean_ba[i, j]\n",
    "            peaks_i = significant_peak(\n",
    "                stacked_shaps[:, i, j],\n",
    "                diff_threshold=diff_threshold,\n",
    "                ptp_threshold=ptp_threshold,\n",
    "                strict=False,\n",
    "            )\n",
    "            n_peaks.append(peaks_i)\n",
    "        return dict(zip(*np.unique(n_peaks, return_counts=True)))\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=get_ncpus()) as executor:\n",
    "        diff_thresholds = np.round(np.linspace(0.01, 0.99, 8), 2)\n",
    "        ptp_threshold_factors = np.round(np.linspace(0, 0.75, 12), 2)\n",
    "\n",
    "        fs = [\n",
    "            executor.submit(get_n_peaks, ptp_threshold_factor, diff_threshold)\n",
    "            for ptp_threshold_factor, diff_threshold in product(\n",
    "                ptp_threshold_factors, diff_thresholds\n",
    "            )\n",
    "        ]\n",
    "        for f in tqdm(\n",
    "            concurrent.futures.as_completed(fs),\n",
    "            total=len(fs),\n",
    "            desc=\"Varying significances\",\n",
    "            smoothing=0,\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "        results = {}\n",
    "        for (f, (ptp_threshold_factor, diff_threshold)) in zip(\n",
    "            fs, product(ptp_threshold_factors, diff_thresholds)\n",
    "        ):\n",
    "            results[(ptp_threshold_factor, diff_threshold)] = f.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_peaks_dict = {}\n",
    "for key, vals in results.items():\n",
    "    n_peaks_dict[key] = {}\n",
    "    all_tuples = list(vals.keys())\n",
    "    all_n_peaks = np.array([len(tup) for tup in all_tuples])\n",
    "    all_n = np.array(list(vals.values()))\n",
    "\n",
    "    for n in np.unique(all_n_peaks):\n",
    "        n_peaks_dict[key][n] = np.sum(all_n[all_n_peaks == n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(n_peaks_dict).T\n",
    "df.index.names = [\"ptp_thres_f\", \"diff_thres\"]\n",
    "_ = df.groupby(\"ptp_thres_f\").plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(n_peaks_dict).T\n",
    "df.index.names = [\"ptp_thres_f\", \"diff_thres\"]\n",
    "_ = df.groupby(\"diff_thres\").plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carry out run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfts = ESA_CCI_Landcover_PFT()\n",
    "pfts.limit_months(start=PartialDateTime(2010, 1), end=PartialDateTime(2015, 1))\n",
    "pfts.regrid()\n",
    "pfts = pfts.get_mean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pft_cube = Datasets(pfts).select_variables(\"pftHerb\", inplace=False).cube\n",
    "fig = cube_plotting(pft_cube, fig=plt.figure(figsize=(12, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pft_cubes = []\n",
    "pft_names = (\"pftShrubBD\", \"pftShrubBE\", \"pftShrubNE\")\n",
    "\n",
    "for pft_name in pft_names:\n",
    "    pft_cube = Datasets(pfts).select_variables(pft_name, inplace=False).cube\n",
    "    pft_cubes.append(pft_cube)\n",
    "    fig = cube_plotting(pft_cube, fig=plt.figure(figsize=(12, 5)))\n",
    "fig = cube_plotting(\n",
    "    reduce(lambda x, y: x + y, pft_cubes),\n",
    "    title=f\"Sum of {', '.join(pft_names)}\",\n",
    "    fig=plt.figure(figsize=(12, 5)),\n",
    ")\n",
    "\n",
    "pft_cube = Datasets(pfts).select_variables(\"ShrubAll\", inplace=False).cube\n",
    "fig = cube_plotting(pft_cube, fig=plt.figure(figsize=(12, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pft_cubes = []\n",
    "pft_names = (\"pftTreeBD\", \"pftTreeBE\", \"pftTreeND\", \"pftTreeNE\")\n",
    "for pft_name in pft_names:\n",
    "    pft_cube = Datasets(pfts).select_variables(pft_name, inplace=False).cube\n",
    "    pft_cubes.append(pft_cube)\n",
    "    fig = cube_plotting(pft_cube, fig=plt.figure(figsize=(12, 5)))\n",
    "fig = cube_plotting(\n",
    "    reduce(lambda x, y: x + y, pft_cubes),\n",
    "    title=f\"Sum of {', '.join(pft_names)}\",\n",
    "    fig=plt.figure(figsize=(12, 5)),\n",
    ")\n",
    "\n",
    "pft_cube = Datasets(pfts).select_variables(\"TreeAll\", inplace=False).cube\n",
    "fig = cube_plotting(pft_cube, fig=plt.figure(figsize=(12, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pft_cubes = []\n",
    "pft_names = (\"ShrubAll\", \"TreeAll\")\n",
    "for pft_name in pft_names:\n",
    "    pft_cube = Datasets(pfts).select_variables(pft_name, inplace=False).cube\n",
    "    pft_cubes.append(pft_cube)\n",
    "    fig = cube_plotting(pft_cube, fig=plt.figure(figsize=(12, 5)))\n",
    "fig = cube_plotting(\n",
    "    reduce(lambda x, y: x + y, pft_cubes),\n",
    "    title=f\"Sum of {', '.join(pft_names)}\",\n",
    "    fig=plt.figure(figsize=(12, 5)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_month = 9\n",
    "close_figs = True\n",
    "\n",
    "\n",
    "def param_iter():\n",
    "    for (exc_name, exclude_inst) in tqdm(\n",
    "        [(\"with_inst\", False), (\"no_inst\", True)], desc=\"Exclude inst.\"\n",
    "    ):\n",
    "        for feature_name in tqdm(\n",
    "            [\"VOD Ku-band\", \"SIF\", \"FAPAR\", \"LAI\", \"Dry Day Period\"], desc=\"Feature\"\n",
    "        ):\n",
    "            for (filter_name, shap_results) in tqdm(\n",
    "                [(\"normal\", map_shap_results), (\"high_ba\", high_ba_map_shap_results)],\n",
    "                desc=\"SHAP results\",\n",
    "            ):\n",
    "                for shap_measure in tqdm(\n",
    "                    [\n",
    "                        \"masked_max_shap_arrs\",\n",
    "                        \"masked_abs_shap_arrs\",\n",
    "                        \"masked_shap_arrs\",\n",
    "                    ],\n",
    "                    desc=\"SHAP measure\",\n",
    "                ):\n",
    "                    yield (exc_name, exclude_inst), feature_name, (\n",
    "                        filter_name,\n",
    "                        shap_results,\n",
    "                    ), shap_measure\n",
    "\n",
    "\n",
    "for (\n",
    "    (exc_name, exclude_inst),\n",
    "    feature_name,\n",
    "    (filter_name, shap_results),\n",
    "    shap_measure,\n",
    ") in islice(param_iter(), 0, None):\n",
    "    short_feature = shorten_features(feature_name)\n",
    "\n",
    "    sub_directory = (\n",
    "        Path(\"shap_peaks\") / filter_name / shap_measure / short_feature / exc_name\n",
    "    )\n",
    "\n",
    "    filtered = np.array(filter_by_month(X_train.columns, feature_name, max_month))\n",
    "    lags = np.array(\n",
    "        [get_lag(feature, target_feature=feature_name) for feature in filtered]\n",
    "    )\n",
    "\n",
    "    # Ensure lags are sorted consistently.\n",
    "    lag_sort_inds = np.argsort(lags)\n",
    "    filtered = tuple(filtered[lag_sort_inds])\n",
    "    lags = tuple(lags[lag_sort_inds])\n",
    "\n",
    "    if exclude_inst and 0 in lags:\n",
    "        assert lags[0] == 0\n",
    "        lags = lags[1:]\n",
    "        filtered = filtered[1:]\n",
    "\n",
    "    n_features = len(filtered)\n",
    "\n",
    "    # There is no point plotting this map for a single feature or less since we are\n",
    "    # interested in a comparison between different feature ranks.\n",
    "    if n_features <= 1:\n",
    "        continue\n",
    "\n",
    "    selected_data = np.empty(n_features, dtype=object)\n",
    "    for i, col in enumerate(X_train.columns):\n",
    "        if col in filtered:\n",
    "            selected_data[lags.index(get_lag(col))] = shap_results[shap_measure][\n",
    "                \"data\"\n",
    "            ][i].copy()\n",
    "\n",
    "    shared_mask = reduce(np.logical_or, (data.mask for data in selected_data))\n",
    "    for data in selected_data:\n",
    "        data.mask = shared_mask\n",
    "\n",
    "    stacked_shaps = np.vstack([data.data[np.newaxis] for data in selected_data])\n",
    "\n",
    "    # Calculate the significance of the global maxima for each of the valid pixels.\n",
    "\n",
    "    # Valid indices are recorded in 'shared_mask'.\n",
    "\n",
    "    valid_i, valid_j = np.where(~shared_mask)\n",
    "    total_valid = len(valid_i)\n",
    "\n",
    "    max_positions = np.ma.MaskedArray(\n",
    "        np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "    )\n",
    "\n",
    "    peak_indices = []\n",
    "\n",
    "    for i, j in zip(tqdm(valid_i, desc=\"Evaluating maxima\", smoothing=0), valid_j):\n",
    "        ptp_threshold = ptp_threshold_factor * mean_ba[i, j]\n",
    "        peaks_i = significant_peak(\n",
    "            stacked_shaps[:, i, j],\n",
    "            diff_threshold=diff_threshold,\n",
    "            ptp_threshold=ptp_threshold,\n",
    "            strict=False,\n",
    "        )\n",
    "\n",
    "        # Disregarding the sign of the mean influence, sorted by absolute\n",
    "        # value (not peak height) magnitude.\n",
    "        #\n",
    "        # peak_indices.append(peaks_i)\n",
    "\n",
    "        # Adding information about the sign of the mean influence, sorted by absolute\n",
    "        # value (not peak height) magnitude.\n",
    "        #\n",
    "        #         peak_indices.append(tuple(\n",
    "        #             f\"{p_i}({'+' if stacked_shaps[p_i, i, j] > 0 else '-'})\" for p_i in peaks_i\n",
    "        #         ))\n",
    "\n",
    "        # Adding information about the sign of the mean influence, sorted by time.\n",
    "        #\n",
    "        peak_indices.append(\n",
    "            tuple(\n",
    "                f\"{lags[p_i]}({'+' if stacked_shaps[p_i, i, j] > 0 else '-'})\"\n",
    "                for p_i in sorted(peaks_i)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #\n",
    "    pd.Series(\n",
    "        dict(\n",
    "            zip(\n",
    "                *np.unique(\n",
    "                    [len(indices) for indices in peak_indices], return_counts=True\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ).plot.bar(\n",
    "        ax=plt.subplots(figsize=(6, 4))[1],\n",
    "        title=f\"{short_feature}, exclude inst: {exclude_inst}\",\n",
    "        rot=0,\n",
    "    )\n",
    "    figure_saver.save_figure(plt.gcf(), \"n_peaks_distr\", sub_directory=sub_directory)\n",
    "    if close_figs:\n",
    "        plt.close()\n",
    "    #\n",
    "\n",
    "    peaks_arr = np.ma.MaskedArray(\n",
    "        np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "    )\n",
    "    for i, j, indices in zip(valid_i, valid_j, peak_indices):\n",
    "        peaks_arr[i, j] = len(indices)\n",
    "\n",
    "    #\n",
    "    fig, cbar = cube_plotting(\n",
    "        peaks_arr,\n",
    "        title=f\"Nr. Peaks {short_feature}, exclude inst: {exclude_inst}\",\n",
    "        boundaries=np.arange(0, 4) - 0.5,\n",
    "        fig=plt.figure(figsize=(5.1, 2.6)),\n",
    "        coastline_kwargs={\"linewidth\": 0.3},\n",
    "        colorbar_kwargs={\"label\": \"nr. peaks\", \"format\": \"%0.1f\"},\n",
    "        return_cbar=True,\n",
    "    )\n",
    "\n",
    "    tick_pos = np.arange(4, dtype=np.float64)\n",
    "    tick_pos[3] -= 0.5\n",
    "\n",
    "    cbar.set_ticks(tick_pos)\n",
    "\n",
    "    tick_labels = list(map(str, range(3))) + [\">2\"]\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    #     plt.gca().gridlines()\n",
    "\n",
    "    map_figure_saver.save_figure(\n",
    "        fig, f\"nr_shap_peaks_map_{short_feature}\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()\n",
    "    #\n",
    "\n",
    "    masked_peaks = peaks_arr.copy()\n",
    "    masked_peaks.mask |= (peaks_arr.data == 0) | (peaks_arr.data > 2)\n",
    "\n",
    "    cmap, norm = from_levels_and_colors(\n",
    "        levels=np.arange(1, 4) - 0.5, colors=[\"C1\", \"C2\"], extend=\"neither\",\n",
    "    )\n",
    "\n",
    "    #\n",
    "    fig, cbar = cube_plotting(\n",
    "        masked_peaks,\n",
    "        title=f\"Nr. Peaks {short_feature}, exclude inst: {exclude_inst}\",\n",
    "        # boundaries=np.arange(1, 4) - 0.5,\n",
    "        fig=plt.figure(figsize=(5.1, 2.6)),\n",
    "        coastline_kwargs={\"linewidth\": 0.3},\n",
    "        colorbar_kwargs={\"label\": \"nr. peaks\", \"format\": \"%0.1f\"},\n",
    "        return_cbar=True,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "    )\n",
    "\n",
    "    tick_pos = np.arange(3, dtype=np.float64)\n",
    "    cbar.set_ticks(tick_pos)\n",
    "\n",
    "    tick_labels = list(map(str, range(1, 3)))\n",
    "    cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "    #     plt.gca().gridlines()\n",
    "\n",
    "    map_figure_saver.save_figure(\n",
    "        fig, f\"filtered_nr_shap_peaks_map_{short_feature}\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()\n",
    "    #\n",
    "\n",
    "    valid_peak_indices = []\n",
    "    for i, j, peaks_i in zip(valid_i, valid_j, peak_indices):\n",
    "        if masked_peaks.mask[i, j]:\n",
    "            # Only use valid samples.\n",
    "            continue\n",
    "        valid_peak_indices.append(peaks_i)\n",
    "\n",
    "    assert np.all(\n",
    "        np.sort(np.unique([len(indices) for indices in valid_peak_indices]))\n",
    "        == np.array([1, 2])\n",
    "    )\n",
    "\n",
    "    #\n",
    "    pd.Series(\n",
    "        dict(\n",
    "            zip(\n",
    "                *np.unique(\n",
    "                    [len(indices) for indices in valid_peak_indices], return_counts=True\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ).plot.bar(\n",
    "        ax=plt.subplots(figsize=(6, 4))[1],\n",
    "        title=f\"{short_feature}, exclude inst: {exclude_inst}\",\n",
    "        rot=0,\n",
    "    )\n",
    "    figure_saver.save_figure(\n",
    "        plt.gcf(), \"filtered_n_peaks_distr\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()\n",
    "    #\n",
    "\n",
    "    peaks_dict = dict(zip(*np.unique(valid_peak_indices, return_counts=True)))\n",
    "\n",
    "    total_counts = np.sum(list(peaks_dict.values()))\n",
    "    relative_counts_dict = {key: val / total_counts for key, val in peaks_dict.items()}\n",
    "    #     print(f\"{short_feature}, exclude inst: {exclude_inst}\", relative_counts_dict)\n",
    "\n",
    "    #\n",
    "    fig = plt.figure(figsize=(7, 0.3 * len(relative_counts_dict) + 0.4))\n",
    "    pd.Series(\n",
    "        {\", \".join(k): v for k, v in relative_counts_dict.items()}\n",
    "    ).sort_values().plot.barh(\n",
    "        fontsize=12, title=f\"{short_feature}, exclude inst: {exclude_inst}\",\n",
    "    )\n",
    "    figure_saver.save_figure(plt.gcf(), \"peak_comb_distr\", sub_directory=sub_directory)\n",
    "    if close_figs:\n",
    "        plt.close()\n",
    "    #\n",
    "\n",
    "    ##### Eliminate the lowest X% iff there are more than Y combinations\n",
    "\n",
    "    keys, values = list(zip(*relative_counts_dict.items()))\n",
    "    keys = np.asarray(keys)\n",
    "    values = np.asarray(values)\n",
    "\n",
    "    #     elim_frac = 0.2\n",
    "    #     min_n_peaks = 6\n",
    "    max_n_peaks = 6\n",
    "    min_frac = 0.05  # Require at least this fraction per entry.\n",
    "\n",
    "    sorted_indices = np.argsort(values)\n",
    "    cumulative_fractions = np.cumsum(values[sorted_indices])\n",
    "\n",
    "    # Ensure at least `min_n_entries` entries are present, but no more than `max_n_peaks`.\n",
    "    mask = np.ones_like(cumulative_fractions, dtype=np.bool_)\n",
    "    #     mask[-min_n_peaks:] = True\n",
    "    mask[:-max_n_peaks] = False  # no more than `max_n_peaks`.\n",
    "\n",
    "    #     mask |= (cumulative_fractions > elim_frac)\n",
    "    mask &= values[sorted_indices] > min_frac\n",
    "\n",
    "    print(\n",
    "        f\"Remaining fraction: {short_feature}, exclude inst: {exclude_inst}\",\n",
    "        np.sum(values[sorted_indices][mask]),\n",
    "    )\n",
    "\n",
    "    thres_counts_dict = {\n",
    "        key: val\n",
    "        for key, val in zip(keys[sorted_indices][mask], values[sorted_indices][mask])\n",
    "    }\n",
    "    print(f\"{short_feature}, exclude inst: {exclude_inst}\", thres_counts_dict)\n",
    "\n",
    "    peak_keys = list(thres_counts_dict)\n",
    "\n",
    "    imp_peaks = np.ma.MaskedArray(\n",
    "        np.zeros_like(shared_mask, dtype=np.float64), mask=True\n",
    "    )\n",
    "\n",
    "    for i, j, indices in zip(valid_i, valid_j, peak_indices):\n",
    "        if indices in peak_keys:\n",
    "            imp_peaks[i, j] = peak_keys.index(indices)\n",
    "\n",
    "    #\n",
    "    boundaries = np.arange(len(peak_keys) + 1) - 0.5\n",
    "\n",
    "    cmap, norm = from_levels_and_colors(\n",
    "        levels=boundaries,\n",
    "        colors=[plt.get_cmap(\"tab10\")(i) for i in range(len(peak_keys))],\n",
    "        extend=\"neither\",\n",
    "    )\n",
    "\n",
    "    fig, cbar = cube_plotting(\n",
    "        imp_peaks,\n",
    "        title=f\"Peak Distr. {short_feature}, exclude inst: {exclude_inst}\",\n",
    "        fig=plt.figure(figsize=(5.1, 2.6)),\n",
    "        coastline_kwargs={\"linewidth\": 0.3},\n",
    "        colorbar_kwargs={\"label\": \"peak combination\"},\n",
    "        return_cbar=True,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "    )\n",
    "\n",
    "    tick_pos = np.arange(len(peak_keys), dtype=np.float64)\n",
    "    cbar.set_ticks(tick_pos)\n",
    "\n",
    "    cbar.set_ticklabels(peak_keys)\n",
    "\n",
    "    #     plt.gca().gridlines()\n",
    "\n",
    "    map_figure_saver.save_figure(\n",
    "        fig, f\"shap_peak_distr_map_{short_feature}\", sub_directory=sub_directory\n",
    "    )\n",
    "    if close_figs:\n",
    "        plt.close()\n",
    "    #\n",
    "\n",
    "    assert len(np.unique(imp_peaks.data[~imp_peaks.mask])) == len(peak_keys)\n",
    "\n",
    "    results = {}\n",
    "    for comb_i in tqdm(\n",
    "        np.unique(imp_peaks.data[~imp_peaks.mask]), desc=\"Peak combination\"\n",
    "    ):\n",
    "        for pft_cube in pfts:\n",
    "            selection = (~(pft_cube.data.mask | imp_peaks.mask)) & np.isclose(\n",
    "                imp_peaks, comb_i\n",
    "            )\n",
    "            results[\n",
    "                (str(peak_keys[int(comb_i)]), pft_cube.name())\n",
    "            ] = pft_cube.data.data[selection]\n",
    "\n",
    "    df = pd.DataFrame({key: pd.Series(vals) for key, vals in results.items()})\n",
    "    df.columns.names = [\"peak_combination\", \"pft\"]\n",
    "\n",
    "    for level in [0, 1]:\n",
    "        #\n",
    "        df.groupby(axis=1, level=level).boxplot(\n",
    "            subplots=True,\n",
    "            layout=(len(df.columns.levels[level]), 1),\n",
    "            figsize=(10, 5 * len(df.columns.levels[level])),\n",
    "            rot=30,\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        figure_saver.save_figure(\n",
    "            plt.gcf(),\n",
    "            f\"boxplots_level_{df.columns.names[level]}\",\n",
    "            sub_directory=sub_directory,\n",
    "        )\n",
    "        if close_figs:\n",
    "            plt.close()\n",
    "        #\n",
    "\n",
    "        for key in df.columns.levels[level]:\n",
    "            #\n",
    "            fig = plt.figure()\n",
    "            sns.violinplot(data=df.xs(key, level=level, axis=\"columns\"))\n",
    "            plt.title(key)\n",
    "            figure_saver.save_figure(\n",
    "                fig,\n",
    "                f\"violin_level_{df.columns.names[level]}_{key}\",\n",
    "                sub_directory=sub_directory,\n",
    "            )\n",
    "            if close_figs:\n",
    "                plt.close()\n",
    "            #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires] *",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
