{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter RF Optimisation for Lagged Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from operator import mul\n",
    "\n",
    "import cloudpickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import shap\n",
    "from joblib import Memory, Parallel, delayed\n",
    "from loguru import logger as loguru_logger\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wildfires.analysis\n",
    "from alepython import ale_plot\n",
    "from alepython.ale import _second_order_ale_quant\n",
    "from wildfires.analysis import *\n",
    "from wildfires.dask_cx1 import *\n",
    "from wildfires.data import *\n",
    "from wildfires.logging_config import enable_logging\n",
    "from wildfires.qstat import get_ncpus\n",
    "from wildfires.utils import *\n",
    "\n",
    "loguru_logger.enable(\"alepython\")\n",
    "loguru_logger.remove()\n",
    "loguru_logger.add(sys.stderr, level=\"WARNING\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "enable_logging(\"jupyter\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Collapsing a non-contiguous coordinate.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*DEFAULT_SPHERICAL_EARTH_RADIUS*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*guessing contiguous bounds*\")\n",
    "\n",
    "normal_coast_linewidth = 0.5\n",
    "mpl.rc(\"figure\", figsize=(14, 6))\n",
    "mpl.rc(\"font\", size=9.0)\n",
    "\n",
    "save_name = \"analysis_lags_rf_cross_val\"\n",
    "\n",
    "figure_saver = FigureSaver(directories=os.path.join(\"~\", \"tmp\", save_name), debug=True,)\n",
    "memory = get_memory(save_name, verbose=100)\n",
    "CACHE_DIR = os.path.join(DATA_DIR, \".pickle\", save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the customized `get_data()` function for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_lags_rf_cross_val_data import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data Structures used for Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_months = [1, 3, 6, 9, 12, 18, 24]\n",
    "\n",
    "(\n",
    "    e_s_endog_data,\n",
    "    e_s_exog_data,\n",
    "    e_s_master_mask,\n",
    "    e_s_filled_datasets,\n",
    "    e_s_masked_datasets,\n",
    "    e_s_land_mask,\n",
    ") = get_data(shift_months=shift_months, selection_variables=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset data from 12 or more months before the current month in order to ease analysis (interpretability).\n",
    "\n",
    "We are interested in the trends in these properties, not their absolute values, therefore we subtract a recent 'seasonal cycle' analogue.\n",
    "This hopefully avoids capturing the same relationships for a variable and its 12 month counterpart due to their high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = []\n",
    "for column in e_s_exog_data:\n",
    "    match = re.search(r\"-\\d{1,2}\", column)\n",
    "    if match:\n",
    "        span = match.span()\n",
    "        # Change the string to reflect the shift.\n",
    "        original_offset = int(column[slice(*span)])\n",
    "        if original_offset > -12:\n",
    "            # Only shift months that are 12 or more months before the current month.\n",
    "            continue\n",
    "        comp = -(-original_offset % 12)\n",
    "        new_column = \" \".join(\n",
    "            (\n",
    "                column[: span[0] - 1],\n",
    "                f\"{original_offset} - {comp}\",\n",
    "                column[span[1] + 1 :],\n",
    "            )\n",
    "        )\n",
    "        if comp == 0:\n",
    "            comp_column = column[: span[0] - 1]\n",
    "        else:\n",
    "            comp_column = \" \".join(\n",
    "                (column[: span[0] - 1], f\"{comp}\", column[span[1] + 1 :])\n",
    "            )\n",
    "        print(column, comp_column)\n",
    "        e_s_exog_data[new_column] = e_s_exog_data[column] - e_s_exog_data[comp_column]\n",
    "        to_delete.append(column)\n",
    "for column in to_delete:\n",
    "    del e_s_exog_data[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimisation Using Dask on CX1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "from dask.distributed import worker_client\n",
    "from joblib import parallel_backend\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#  Does spread out training of individual trees, but then fails with CancelledError...\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# This works but only allocates one thread per Forest.\n",
    "# from dask_ml.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, wait\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "def fit_and_score(X, y, train_index, test_index, rf_params):\n",
    "    with parallel_backend(\"dask\"):\n",
    "        rf = RandomForestRegressor(**rf_params)\n",
    "\n",
    "        rf.fit(X[train_index], y[train_index])\n",
    "\n",
    "        test_score = rf.score(X[test_index], y[test_index])\n",
    "        train_score = rf.score(X[train_index], y[train_index])\n",
    "\n",
    "    return test_score, train_score\n",
    "\n",
    "\n",
    "# Define the training and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    e_s_exog_data, e_s_endog_data, random_state=1, shuffle=True, test_size=0.3\n",
    ")\n",
    "\n",
    "# Define the parameter space.\n",
    "\n",
    "# parameters_RF = {\n",
    "#     \"n_estimators\": [100, 500, 1000],\n",
    "#     \"max_depth\": [5, 9, 12],\n",
    "#     \"min_samples_split\": [2, 5, 10],\n",
    "#     \"min_samples_leaf\": [1, 4],\n",
    "#     \"max_leaf_nodes\": [500, 1500, None],\n",
    "# }\n",
    "\n",
    "parameters_RF = {\n",
    "    \"n_estimators\": [500],\n",
    "    \"max_depth\": [9, 12],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"max_leaf_nodes\": [500, 2000],\n",
    "}\n",
    "\n",
    "default_param_dict = {\n",
    "    \"random_state\": 1,\n",
    "    \"bootstrap\": True,\n",
    "    \"max_features\": \"auto\",\n",
    "}\n",
    "\n",
    "rf_params_list = [\n",
    "    dict(zip(parameters_RF, param_values))\n",
    "    for param_values in product(*parameters_RF.values())\n",
    "]\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "cross_val_cache2 = SimpleCache(\"rf_cross_val2\", cache_dir=CACHE_DIR)\n",
    "\n",
    "# XXX:\n",
    "cross_val_cache2.clear()\n",
    "\n",
    "\n",
    "@cross_val_cache2\n",
    "def run_cross_val2():\n",
    "    #     X = X_train[:int(5e4)].values\n",
    "    #     y = y_train[:int(5e4)].values\n",
    "\n",
    "    X = X_train.values\n",
    "    y = y_train.values\n",
    "\n",
    "    test_scores_list = []\n",
    "    train_scores_list = []\n",
    "\n",
    "    rf_params = default_param_dict.copy()\n",
    "\n",
    "    for rf_grid_params in tqdm(rf_params_list, desc=\"Grid\"):\n",
    "        rf_params.update(rf_grid_params)\n",
    "\n",
    "        test_scores = []\n",
    "        train_scores = []\n",
    "        for train_index, test_index in tqdm(list(kf.split(X)), desc=\"Splits\"):\n",
    "            test_score, train_score = fit_and_score(\n",
    "                X, y, train_index, test_index, rf_params\n",
    "            )\n",
    "            test_scores.append(test_score)\n",
    "            train_scores.append(train_score)\n",
    "\n",
    "        test_scores_list.append(test_scores)\n",
    "        train_scores_list.append(train_scores)\n",
    "\n",
    "    mean_test_scores = [np.mean(scores) for scores in test_scores_list]\n",
    "    best_params = rf_params_list[np.argmin(mean_test_scores)]\n",
    "    print(\"Best parameters:\", best_parameters)\n",
    "\n",
    "    rf_params.update(best_params)\n",
    "    est = RandomForestRegressor(**rf_params)\n",
    "    with parallel_backend(\"dask\"):\n",
    "        est.fit(X_train.values, y_train.values)\n",
    "\n",
    "    return est, test_scores_list, train_scores_list\n",
    "\n",
    "\n",
    "est, test_scores_list, train_scores_list = run_cross_val2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best model parameters using the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_dict = {\"score\": [], \"parameter_index\": [], \"type\": []}\n",
    "for i, (test_scores, train_scores) in enumerate(\n",
    "    zip(test_scores_list, train_scores_list)\n",
    "):\n",
    "    n_split = len(train_scores)\n",
    "    assert n_split == len(test_scores)\n",
    "\n",
    "    vis_dict[\"parameter_index\"].extend([i] * n_split * 2)\n",
    "\n",
    "    vis_dict[\"score\"].extend(test_scores)\n",
    "    vis_dict[\"type\"].extend([\"Test\"] * n_split)\n",
    "\n",
    "    vis_dict[\"score\"].extend(train_scores)\n",
    "    vis_dict[\"type\"].extend([\"Train\"] * n_split)\n",
    "\n",
    "vis_df = pd.DataFrame(vis_dict)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.rc(\"figure\", figsize=(7, 5))\n",
    "with figure_saver(\"hyperparam_opt_small\"):\n",
    "    ax = sns.boxplot(x=\"parameter_index\", y=\"score\", hue=\"type\", data=vis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter_index, parameter_values in enumerate(rf_params_list):\n",
    "    print(\"Index:\", parameter_index)\n",
    "    print(parameter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Does spread out training of individual trees, but then FAILS with CancelledError, or spends forever on the last step...\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # This works, but only seems to allocate one thread PER FOREST.\n",
    "# # from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the training and test data.\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     e_s_exog_data, e_s_endog_data, random_state=1, shuffle=True, test_size=0.3\n",
    "# )\n",
    "\n",
    "# # Define the parameter space.\n",
    "\n",
    "# # parameters_RF = {\n",
    "# #     \"n_estimators\": [100, 500, 1000],\n",
    "# #     \"max_depth\": [5, 9, 12],\n",
    "# #     \"min_samples_split\": [2, 5, 10],\n",
    "# #     \"min_samples_leaf\": [1, 4],\n",
    "# #     \"max_leaf_nodes\": [500, 1500, None],\n",
    "# # }\n",
    "\n",
    "# parameters_RF = {\n",
    "#     \"n_estimators\": [100, 500],\n",
    "#     \"max_depth\": [9, 12],\n",
    "#     \"min_samples_split\": [2, 4],\n",
    "#     \"min_samples_leaf\": [1, 4],\n",
    "#     \"max_leaf_nodes\": [500, 2000],\n",
    "# }\n",
    "\n",
    "# rf_params_list = [dict(zip(parameters_RF, param_values)) for param_values in product(*parameters_RF.values())]\n",
    "\n",
    "# opt_cache = SimpleCache(\"rf_opt\", cache_dir=CACHE_DIR)\n",
    "\n",
    "# # XXX:\n",
    "# opt_cache.clear()\n",
    "\n",
    "# @opt_cache\n",
    "# def grid_search():\n",
    "# #     X = X_train[:int(5e4)].values\n",
    "# #     y = y_train[:int(5e4)].values\n",
    "\n",
    "#     X = X_train.values\n",
    "#     y = y_train.values\n",
    "\n",
    "#     gs = GridSearchCV(\n",
    "#         RandomForestRegressor(\n",
    "#             random_state=1,\n",
    "#             bootstrap=True,\n",
    "#             max_features='auto',\n",
    "#         ),\n",
    "#         parameters_RF,\n",
    "# #         scheduler=client,\n",
    "#         return_train_score=True,\n",
    "#         refit=False,\n",
    "#         cv=3,\n",
    "#     )\n",
    "\n",
    "#     with parallel_backend(\"dask\", scatter=[X, y]):  # Is this needed?\n",
    "#         gs.fit(X, y)\n",
    "# #         gs.fit(X_train, y_train)\n",
    "\n",
    "#     return gs\n",
    "\n",
    "# gs = grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import describe\n",
    "\n",
    "print(describe([tree.get_n_leaves() for tree in est.estimators_]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
