{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from specific import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get shifted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    endog_data,\n",
    "    exog_data,\n",
    "    master_mask,\n",
    "    filled_datasets,\n",
    "    masked_datasets,\n",
    "    land_mask,\n",
    ") = get_offset_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@data_split_cache\n",
    "def get_split_data():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        exog_data, endog_data, random_state=1, shuffle=True, test_size=0.3\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific model training without grid seach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "param_dict = {\n",
    "    \"random_state\": 1,\n",
    "    \"bootstrap\": True,\n",
    "    \"ccp_alpha\": 0.0,\n",
    "    \"max_depth\": 18,\n",
    "    \"max_features\": \"auto\",\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"n_estimators\": 500,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cached results only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = CachedResults(\n",
    "    estimator_class=DaskRandomForestRegressor, n_splits=n_splits, cache_dir=CACHE_DIR\n",
    ")\n",
    "results = cached.collate_scores(train_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DaskRandomForestRegressor(**param_dict)\n",
    "model.n_jobs = 32\n",
    "model_key = tuple(sorted(model.get_params().items()))\n",
    "try:\n",
    "    model = cached.get_estimator(model_key)\n",
    "except KeyError:\n",
    "    with parallel_backend(\"dask\"):\n",
    "        model.fit(X_train, y_train)\n",
    "    cached.store_estimator(model_key, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Place into expected cache location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cross_val_cache\n",
    "def dummy_f():\n",
    "    return {}, model\n",
    "\n",
    "\n",
    "_, model1 = dummy_f()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "# Define the parameter space.\n",
    "\n",
    "# 1024 combinations ([100, 200] est., x 5 splits) takes ~ 20 hrs.\n",
    "\n",
    "parameters_RF = {\n",
    "    \"n_estimators\": [300, 500],\n",
    "    \"max_depth\": [14, 18],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"auto\"],\n",
    "    \"ccp_alpha\": np.linspace(0, 4e-9, 10),\n",
    "}\n",
    "\n",
    "default_param_dict = {\n",
    "    \"random_state\": 1,\n",
    "    \"bootstrap\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, rf = fit_dask_sub_est_random_search_cv(\n",
    "    DaskRandomForestRegressor(**default_param_dict),\n",
    "    X_train.values,\n",
    "    y_train.values,\n",
    "    parameters_RF,\n",
    "    client,\n",
    "    n_splits=n_splits,\n",
    "    max_time=\"24h\",\n",
    "    n_iter=None,\n",
    "    verbose=True,\n",
    "    return_train_score=True,\n",
    "    refit=True,\n",
    "    local_n_jobs=30,\n",
    "    random_state=0,\n",
    "    cache_dir=CACHE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = defaultdict(list)\n",
    "\n",
    "for param_tuples, param_results in results.items():\n",
    "    for category, scores in param_results.items():\n",
    "        if len(scores) == n_splits:\n",
    "            hyperparams[category].append(np.mean(scores))\n",
    "            hyperparams[category + \"_std\"].append(np.std(scores))\n",
    "        else:\n",
    "            print(param_tuples, category, len(scores))\n",
    "            break  # Do not append anything.\n",
    "    else:\n",
    "        for param, param_value in param_tuples:\n",
    "            hyperparams[param].append(param_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = pd.DataFrame(hyperparams)\n",
    "score_keys = list(param_results)\n",
    "score_std_keys = [score_key + \"_std\" for score_key in score_keys]\n",
    "param_keys = list(set(hyperparams.columns) - set(score_keys) - set(score_std_keys))\n",
    "hyperparams.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_gap = hyperparams[hyperparams[\"test_score\"] > 0.64].copy()\n",
    "hyperparams_gap[\"gap\"] = hyperparams_gap[\"train_score\"] - hyperparams_gap[\"test_score\"]\n",
    "print(len(hyperparams_gap))\n",
    "hyperparams_gap.sort_values(by=\"gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams.sort_values(by=\"test_score\", ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams.boxplot(column=score_keys, by=[\"min_samples_split\", \"n_estimators\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted = pd.melt(\n",
    "    hyperparams[hyperparams[\"test_score\"] > 0.65].drop(columns=score_std_keys),\n",
    "    id_vars=param_keys,\n",
    "    value_vars=score_keys,\n",
    "    var_name=\"category\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "melted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the effect of individual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alepython.ale import _sci_format\n",
    "\n",
    "for param_key in param_keys:\n",
    "    if param_key == \"ccp_alpha\":\n",
    "        fig = plt.figure(figsize=(25, 6))\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(9, 6))\n",
    "\n",
    "    ax = sns.boxplot(x=param_key, y=\"score\", hue=\"category\", data=melted)\n",
    "    ax.set(ylabel=\"R2 Score\")\n",
    "    ax.grid(which=\"both\", alpha=0.4, linestyle=\"--\")\n",
    "\n",
    "    if param_key == \"ccp_alpha\":\n",
    "        ax.xaxis.set_ticklabels(\n",
    "            _sci_format(\n",
    "                np.array(\n",
    "                    list(map(lambda x: float(x.get_text()), ax.xaxis.get_ticklabels()))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        ax.xaxis.set_tick_params(rotation=45)\n",
    "\n",
    "    figure_saver.save_figure(fig, param_key, sub_directory=\"hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for the standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_std = pd.melt(\n",
    "    hyperparams[hyperparams[\"test_score\"] > 0.65].drop(columns=score_keys),\n",
    "    id_vars=param_keys,\n",
    "    value_vars=score_std_keys,\n",
    "    var_name=\"category\",\n",
    "    value_name=\"score_std\",\n",
    ")\n",
    "melted_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the effect of individual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alepython.ale import _sci_format\n",
    "\n",
    "for param_key in param_keys:\n",
    "    if param_key == \"ccp_alpha\":\n",
    "        fig = plt.figure(figsize=(25, 6))\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(9, 6))\n",
    "\n",
    "    ax = sns.boxplot(x=param_key, y=\"score_std\", hue=\"category\", data=melted_std)\n",
    "    ax.set(ylabel=\"R2 Score\")\n",
    "    ax.grid(which=\"both\", alpha=0.4, linestyle=\"--\")\n",
    "\n",
    "    if param_key == \"ccp_alpha\":\n",
    "        ax.xaxis.set_ticklabels(\n",
    "            _sci_format(\n",
    "                np.array(\n",
    "                    list(map(lambda x: float(x.get_text()), ax.xaxis.get_ticklabels()))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        ax.xaxis.set_tick_params(rotation=45)\n",
    "\n",
    "    figure_saver.save_figure(fig, param_key, sub_directory=\"hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence of R2 gap on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = hyperparams[\"test_score\"] > 0.66\n",
    "gap = hyperparams[mask][\"train_score\"] - hyperparams[mask][\"test_score\"]\n",
    "\n",
    "# colorby = \"max_depth\"\n",
    "for colorby in param_keys:\n",
    "    c = hyperparams[mask][colorby]\n",
    "    try:\n",
    "        np.asarray(c, dtype=np.float64)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    for key in (\"train_score\", \"test_score\")[1:]:\n",
    "        plt.figure()\n",
    "        plt.scatter(hyperparams[mask][key], gap, marker=\"o\", alpha=0.3, c=c)\n",
    "        plt.ylabel(\"R2 train - test\")\n",
    "        plt.xlabel(key)\n",
    "        plt.colorbar(label=colorby)\n",
    "        plt.grid(alpha=0.4, linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scores = {}\n",
    "\n",
    "model.n_jobs = get_ncpus()\n",
    "with parallel_backend(\"threading\", n_jobs=get_ncpus()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores[\"test_r2\"] = r2_score(y_test, y_pred)\n",
    "    scores[\"test_mse\"] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    train_y_pred = model.predict(X_train)\n",
    "    scores[\"train_r2\"] = r2_score(y_train, train_y_pred)\n",
    "    scores[\"train_mse\"] = mean_squared_error(y_train, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hexbin(y_pred, y_test, bins=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(y_pred - y_test, bins=800)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = y_pred - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y_test > 0.01\n",
    "\n",
    "indices = np.argsort(diffs[mask])\n",
    "plt.scatter(\n",
    "    np.arange(len(indices)),\n",
    "    diffs[mask][indices],\n",
    "    marker=\"o\",\n",
    "    rasterized=True,\n",
    "    alpha=0.1,\n",
    "    c=np.log(y_test[mask][indices]),\n",
    ")\n",
    "plt.colorbar(label=\"log(BA Test)\")\n",
    "plt.ylabel(\"Prediction - Observation (test)\")\n",
    "plt.yscale(\"symlog\", linthreshy=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 15))\n",
    "plt.scatter(\n",
    "    np.log10(y_test), diffs, rasterized=True, marker=\"o\", alpha=0.1, c=np.log10(y_pred)\n",
    ")\n",
    "plt.colorbar(label=\"log10(Pred)\")\n",
    "plt.yscale(\"symlog\", linthreshy=0.00001)\n",
    "plt.ylabel(\"Pred - Obs\")\n",
    "plt.xlabel(\"log10 Obs\")\n",
    "plt.title(\"Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diffs = train_y_pred - y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 15))\n",
    "plt.scatter(\n",
    "    np.log10(y_train),\n",
    "    train_diffs,\n",
    "    rasterized=True,\n",
    "    marker=\"o\",\n",
    "    alpha=0.1,\n",
    "    c=np.log10(train_y_pred),\n",
    ")\n",
    "plt.colorbar(label=\"log10(Pred)\")\n",
    "plt.yscale(\"symlog\", linthreshy=0.00001)\n",
    "plt.ylabel(\"Pred - Obs\")\n",
    "plt.xlabel(\"log10 Obs\")\n",
    "plt.title(\"Training Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y_train > 0.01\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.scatter(\n",
    "    np.log10(y_train),\n",
    "    np.log10(train_y_pred),\n",
    "    rasterized=True,\n",
    "    marker=\"o\",\n",
    "    alpha=0.01,\n",
    "    c=np.log10(train_y_pred),\n",
    ")\n",
    "plt.colorbar(label=\"log10(Pred)\")\n",
    "plt.plot(np.log10(y_train), np.log10(y_train))\n",
    "# plt.yscale('symlog', linthreshy=0.00001);\n",
    "plt.ylabel(\"log10 Pred\")\n",
    "plt.xlabel(\"log10 Obs\")\n",
    "plt.title(\"Training Data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires]",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
